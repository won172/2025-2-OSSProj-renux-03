"""ì—¬ëŸ¬ ë°ì´í„°ì…‹ì— ëŒ€í•œ Chroma ì¸ë±ìŠ¤ ë° SQLite DBë¥¼ êµ¬ì¶•í•˜ëŠ” ë°ì´í„° ìˆ˜ì§‘ ë£¨í‹´ìž…ë‹ˆë‹¤."""
from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Tuple
import json
import argparse # Add logging import

import pandas as pd
import re
from sqlalchemy.orm import Session

from src.config import (
    CHUNK_OVERLAP,
    CHUNK_SIZE,
    CHUNKS_DIR,
    DATA_SOURCES,
)
from src.models.embedding import encode_texts
from src.search.hybrid import train_tfidf
from src.utils.preprocess import (
    apply_cleaning,
    normalize_whitespace,
    make_doc_id,
    to_chunks,
)
from src.vectorstore.chroma_client import add_items, reset_collection, upsert_items, get_all_ids, delete_items
from src.database import (
    SessionLocal, engine, init_db,
    Notice, Rule, Schedule, Course, Staff, Chunk
)


@dataclass
class DatasetArtifacts:
    key: str
    collection: str
    chunk_path: Path

    @property
    def csv_path(self) -> Path:
        return self.chunk_path.with_suffix(".csv")


DATASET_ARTIFACTS: Dict[str, DatasetArtifacts] = {
    "notices": DatasetArtifacts(
        key="notices",
        collection="dongguk_notices",
        chunk_path=CHUNKS_DIR / "notices.parquet",
    ),
    "rules": DatasetArtifacts(
        key="rules",
        collection="dongguk_rules",
        chunk_path=CHUNKS_DIR / "rules.parquet",
    ),
    "schedule": DatasetArtifacts(
        key="schedule",
        collection="dongguk_schedule",
        chunk_path=CHUNKS_DIR / "schedule.parquet",
    ),
    "courses": DatasetArtifacts(
        key="courses",
        collection="dongguk_courses",
        chunk_path=CHUNKS_DIR / "courses.parquet",
    ),
    "staff": DatasetArtifacts(
        key="staff",
        collection="dongguk_staff",
        chunk_path=CHUNKS_DIR / "staff.parquet",
    ),
}


def _persist_chunks(key: str, collection: str, chunks_df: pd.DataFrame) -> Tuple[pd.DataFrame, object, object]:
    if chunks_df.empty:
        print(f"âš ï¸ Warning: No chunks generated for {key}")
        return chunks_df, None, None

    embeddings = encode_texts(chunks_df["chunk_text"].tolist())

    # ë©”íƒ€ë°ì´í„° ì¤€ë¹„ (None ì²˜ë¦¬)
    metadatas = chunks_df.drop(columns=["chunk_text"]).to_dict(orient="records")
    metadatas = [{k: (v if v is not None else "") for k, v in m.items()} for m in metadatas]

    # 1. ê¸°ì¡´ ID ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    existing_ids = set(get_all_ids(collection))
    
    # 2. ìƒˆë¡œìš´ ID ëª©ë¡
    new_ids = set(chunks_df["chunk_id"].astype(str))
    
    # 3. ì‚­ì œí•  ID ê³„ì‚° (ê¸°ì¡´ì—ëŠ” ìžˆì—ˆìœ¼ë‚˜ ì´ë²ˆì—” ì—†ëŠ” ê²ƒ)
    ids_to_delete = list(existing_ids - new_ids)
    
    if ids_to_delete:
        print(f"ðŸ—‘ï¸ Deleting {len(ids_to_delete)} obsolete chunks from {collection}")
        delete_items(collection, ids_to_delete)

    # 4. ì¶”ê°€ ë° ì—…ë°ì´íŠ¸ (Upsert)
    upsert_items(
        collection,
        ids=chunks_df["chunk_id"],
        documents=chunks_df["chunk_text"],
        metadatas=metadatas,
        embeddings=embeddings,
    )

    artifacts = DATASET_ARTIFACTS[key]
    artifacts.chunk_path.parent.mkdir(parents=True, exist_ok=True)

    write_path = artifacts.chunk_path
    try:
        # object íƒ€ìž… ë¬¸ì œ ë°©ì§€ ìœ„í•´ string ë³€í™˜
        chunks_df.astype(str).to_parquet(write_path, index=False)
    except Exception:
        write_path = artifacts.csv_path
        chunks_df.to_csv(write_path, index=False, encoding="utf-8-sig")
    artifacts.chunk_path = write_path

    vectorizer, matrix = train_tfidf(key, chunks_df["chunk_text"].tolist())
    return chunks_df, vectorizer, matrix


def _save_chunks_to_sqlite(chunks_df: pd.DataFrame, source_key: str):
    """SQLiteì˜ chunks í…Œì´ë¸”ì— ì €ìž¥í•©ë‹ˆë‹¤."""
    if chunks_df.empty:
        return
    
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ ë° í™•ë³´
    cols = ["chunk_id", "chunk_text", "notice_id", "rule_id", "schedule_id", "course_id", "staff_id"]
    for col in cols:
        if col not in chunks_df.columns:
            chunks_df[col] = None
            
    # ì €ìž¥í•  ë°ì´í„°í”„ë ˆìž„
    to_save = chunks_df[cols].copy()
    
    # í˜¸ì¶œìžê°€ ì´ë¯¸ ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚­ì œí–ˆë‹¤ê³  ê°€ì •
    to_save.to_sql("chunks", con=engine, if_exists="append", index=False)


def _first_nonempty(row, keys: Iterable[str]) -> str:
    for key in keys:
        val = row.get(key, "") if hasattr(row, "get") else row.get(key, "")
        val_str = str(val).strip()
        if val_str and val_str.lower() != "nan":
            return val_str
    return ""


# --- Notices ---

def build_notice_chunks(df: pd.DataFrame) -> pd.DataFrame:
    column = {
        "title": "ì œëª©",
        "content": "ë³¸ë¬¸",
        "date": "ê²Œì‹œì¼",
        "topic": "ê²Œì‹œíŒ",
        "url": "ìƒì„¸URL",
        "attachment": "ì²¨ë¶€íŒŒì¼",
    }

    cleaned = apply_cleaning(df, content_col=column["content"], date_col=column["date"])

    docs: List[dict] = []
    for _, row in cleaned.iterrows():
        text_content = row.get("clean_text", "")
        if not isinstance(text_content, str) or not text_content.strip():
            continue
        
        # ê²Œì‹œíŒ ìœ í˜•ì„ í…ìŠ¤íŠ¸ì— í¬í•¨
        topic_type = row.get(column["topic"], "")
        if topic_type:
            text_content = f"[ê²Œì‹œíŒ: {topic_type}]\n\n{text_content}"
        
        published = row.get("clean_date")
        doc_id = make_doc_id(row.get(column["title"]), row.get(column["topic"]), published)
        
        # attachments ë¦¬ìŠ¤íŠ¸ë¥¼ JSON ë¬¸ìžì—´ë¡œ ë³€í™˜
        raw_attachments = row.get(column["attachment"], [])
        if isinstance(raw_attachments, list):
            attachments_str = json.dumps(raw_attachments, ensure_ascii=False)
        else:
            attachments_str = str(raw_attachments)

        docs.append(
            {
                "doc_id": doc_id,
                "title": row.get(column["title"], ""),
                "text": text_content,
                "topics": row.get(column["topic"], ""),
                "published_at": published or "",
                "url": row.get(column["url"], ""),
                "attachments": attachments_str,
                "source": "notices",
                "notice_id": row.get("db_id"), # DB ID from ingest_notices
            }
        )

    chunks = to_chunks(
        docs,
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        include_title=True,
    )
    return pd.DataFrame(chunks)


def ingest_notices() -> Tuple[pd.DataFrame, object, object]:
    path = DATA_SOURCES["notices"]
    if not path.exists():
        raise FileNotFoundError(f"Notice CSV not found: {path}")

    raw_df = pd.read_csv(path)
    
    session = SessionLocal()
    try:
        # 1. ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
        session.query(Chunk).filter(Chunk.notice_id.isnot(None)).delete()
        session.query(Notice).delete()
        session.commit()
        
        # 2. ì›ë³¸ ë°ì´í„° ì €ìž¥
        notice_objs = []
        # ë‚ ì§œ í¬ë§· í†µì¼
        raw_df["ê²Œì‹œì¼"] = pd.to_datetime(raw_df["ê²Œì‹œì¼"], errors="coerce").dt.strftime("%Y-%m-%d").fillna("")
        
        for _, row in raw_df.iterrows():
            obj = Notice(
                board=row.get("ê²Œì‹œíŒ"),
                title=row.get("ì œëª©"),
                category=row.get("ì¹´í…Œê³ ë¦¬"),
                published_date=row.get("ê²Œì‹œì¼"),
                is_fixed=str(row.get("ìƒë‹¨ê³ ì •")),
                detail_url=row.get("ìƒì„¸URL"),
                content=row.get("ë³¸ë¬¸"),
                attachments=str(row.get("ì²¨ë¶€íŒŒì¼"))
            )
            notice_objs.append(obj)
            
        session.add_all(notice_objs)
        session.commit()
        
        # 3. ID ë§¤í•‘
        raw_df["db_id"] = [obj.id for obj in notice_objs]
        
    finally:
        session.close()

    # 4. ì²­í¬ ìƒì„± ë° ì €ìž¥
    chunks_df = build_notice_chunks(raw_df)
    _save_chunks_to_sqlite(chunks_df, "notices")
    
    return _persist_chunks("notices", DATASET_ARTIFACTS["notices"].collection, chunks_df)


# --- Rules ---

def build_rule_chunks(df: pd.DataFrame) -> pd.DataFrame:
    docs: List[dict] = []
    for _, row in df.iterrows():
        text = _first_nonempty(row, ["text", "ë‚´ìš©", "ë³¸ë¬¸", "article", "ì¡°ë¬¸", "rule_text"])
        if not text:
            continue
        filename = _first_nonempty(row, ["filename", "íŒŒì¼ëª…", "ê·œì •ëª…", "title"])
        rel_dir = _first_nonempty(row, ["relative_dir", "ê²½ë¡œ", "folder"])
        doc_id = make_doc_id("rules", rel_dir, filename or text[:40])
        docs.append(
            {
                "doc_id": doc_id,
                "title": filename or text[:80] or "í•™ì¹™ ë¬¸ì„œ",
                "text": text,
                "topics": "ê·œì •",
                "relative_dir": rel_dir,
                "filename": filename,
                "source": "rules",
                "url": "",
                "published_at": "",
                "rule_id": row.get("db_id"),
            }
        )

    chunks = to_chunks(
        docs,
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        include_title=True,
    )
    return pd.DataFrame(chunks)


def ingest_rules() -> Tuple[pd.DataFrame, object, object]:
    path = DATA_SOURCES["rules"]
    if not path.exists():
        raise FileNotFoundError(f"Rule CSV not found: {path}")

    df = pd.read_csv(path).fillna("").astype(str)
    
    session = SessionLocal()
    try:
        session.query(Chunk).filter(Chunk.rule_id.isnot(None)).delete()
        session.query(Rule).delete()
        session.commit()

        rule_objs = []
        for _, row in df.iterrows():
            text_val = _first_nonempty(row, ["text", "ë‚´ìš©", "ë³¸ë¬¸", "article", "ì¡°ë¬¸", "rule_text"])
            fname = _first_nonempty(row, ["filename", "íŒŒì¼ëª…", "ê·œì •ëª…", "title"])
            rdir = _first_nonempty(row, ["relative_dir", "ê²½ë¡œ", "folder"])
            
            obj = Rule(filename=fname, relative_dir=rdir, full_text=text_val)
            rule_objs.append(obj)
            
        session.add_all(rule_objs)
        session.commit()
        df["db_id"] = [obj.id for obj in rule_objs]
    finally:
        session.close()
        
    chunks_df = build_rule_chunks(df)
    _save_chunks_to_sqlite(chunks_df, "rules")
    return _persist_chunks("rules", DATASET_ARTIFACTS["rules"].collection, chunks_df)


# --- Schedule ---

def build_schedule_chunks(df: pd.DataFrame) -> pd.DataFrame:
    docs: List[dict] = []
    for _, row in df.iterrows():
        # ingest_scheduleì—ì„œ í• ë‹¹í•œ ê°ì²´ ì‚¬ìš©
        obj = row.get("db_object")
        if not obj: continue

        doc_id = make_doc_id("schedule", obj.start_date, obj.end_date, obj.content)
        
        # í•™ì‚¬ì¼ì • í‚¤ì›Œë“œì™€ ë‚ ì§œ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ì— í¬í•¨
        date_str = f"{obj.start_date}"
        if obj.end_date and obj.end_date != obj.start_date:
            date_str += f" ~ {obj.end_date}"
        
        rich_text = f"í•™ì‚¬ì¼ì •: {obj.title}\n\n{obj.content}\n\nê¸°ê°„: {date_str}"
        if obj.department:
            rich_text += f"\n\nì£¼ê´€ë¶€ì„œ: {obj.department}"

        docs.append(
            {
                "doc_id": doc_id,
                "title": obj.title,
                "text": rich_text,
                "schedule_start": obj.start_date,
                "schedule_end": obj.end_date,
                "category": obj.category,
                "department": obj.department,
                "topics": obj.category or "schedule",
                "source": "schedule",
                "url": "",
                "published_at": obj.start_date,
                "schedule_id": obj.id,
            }
        )

    chunks = to_chunks(
        docs,
        chunk_size=CHUNK_SIZE // 2,
        chunk_overlap=CHUNK_OVERLAP // 2,
        include_title=True,
    )
    return pd.DataFrame(chunks)


def ingest_schedule() -> Tuple[pd.DataFrame, object, object]:
    path = DATA_SOURCES["schedule"]
    if not path.exists():
        raise FileNotFoundError(f"Schedule CSV not found: {path}")

    df = pd.read_csv(path).fillna("").astype(str)
    
    session = SessionLocal()
    try:
        session.query(Chunk).filter(Chunk.schedule_id.isnot(None)).delete()
        session.query(Schedule).delete()
        session.commit()

        sch_objs = []
        dept_pattern = re.compile(r"\(ì£¼ê´€ë¶€ì„œ:\s*(.*?)\)")
        
        parsed_objs = []
        for _, row in df.iterrows():
            start_val = _first_nonempty(row, ["start", "start_date", "ì‹œìž‘", "ì‹œìž‘ì¼"])
            end_val = _first_nonempty(row, ["end", "end_date", "ì¢…ë£Œ", "ì¢…ë£Œì¼"])
            category = _first_nonempty(row, ["êµ¬ë¶„", "category", "ë¶„ë¥˜", "0", "ì¹´í…Œê³ ë¦¬"])
            description = _first_nonempty(row, ["ë‚´ìš©", "ì¼ì •", "event", "2", "description"])
            
            if not description:
                parsed_objs.append(None)
                continue
                
            dept_match = dept_pattern.search(description)
            if dept_match:
                department = dept_match.group(1).strip()
                description = dept_pattern.sub("", description).strip()
            else:
                department = _first_nonempty(row, ["ì£¼ê´€ë¶€ì„œ", "department", "ë¶€ì„œ"])
            
            title = description.split("\n")[0]
            obj = Schedule(
                title=title, start_date=start_val, end_date=end_val,
                category=category, department=department, content=description
            )
            sch_objs.append(obj)
            parsed_objs.append(obj)
            
        session.add_all(sch_objs)
        session.commit()
        df["db_object"] = parsed_objs
        
        # Build chunks INSIDE the session block to access lazy-loaded attributes
        chunks_df = build_schedule_chunks(df)
    finally:
        session.close()

    _save_chunks_to_sqlite(chunks_df, "schedule")
    return _persist_chunks("schedule", DATASET_ARTIFACTS["schedule"].collection, chunks_df)


# --- Courses ---

def build_course_chunks(combined: pd.DataFrame) -> pd.DataFrame:
    docs: List[dict] = []
    ignored_exact = {"_source_table", "db_id", "db_object", "major"}
    title_candidates = ["êµ­ë¬¸êµê³¼ëª©ëª…", "ê³¼ëª©ëª…", "course_name", "êµê³¼ëª©ëª…", "title", "êµê³¼ëª©"]
    
    for _, row in combined.iterrows():
        db_id = row.get("db_id")
        title = next((str(row.get(col, "")).strip() for col in title_candidates if str(row.get(col, "")).strip()), "í†µê³„í•™ê³¼ êµê³¼")
        code = str(row.get("í•™ìˆ˜ë²ˆí˜¸", "")).strip()
        doc_id = make_doc_id("courses", code or title, row.get("_source_table"))

        text_parts: List[str] = []
        for col, value in row.items():
            if col in ignored_exact or col.startswith("Unnamed"):
                continue
            value_str = str(value).strip()
            if not value_str:
                continue
            if col in title_candidates:
                text_parts.append(value_str)
            else:
                label = normalize_whitespace(col)
                # ê°œì„¤í•™ê¸° í¬ë§·íŒ… (ì˜ˆ: "2" -> "2í•™ê¸°")
                if label == "ê°œì„¤í•™ê¸°" and value_str in ["1", "2"]:
                    value_str += "í•™ê¸°"
                text_parts.append(f"{label}: {value_str}")
        text = "\n".join(text_parts).strip()
        if not text:
            continue
        
        docs.append(
            {
                "doc_id": doc_id,
                "title": title,
                "text": text,
                "course_code": code,
                "source_table": row.get("_source_table", ""),
                "topics": row.get("_source_table", ""),
                "source": "courses",
                "url": "",
                "published_at": "",
                "course_id": db_id,
                "major": row.get("major", ""), # ë©”íƒ€ë°ì´í„°ì— major ì¶”ê°€
            }
        )

    chunks = to_chunks(
        docs,
        chunk_size=None, 
        chunk_overlap=0,
        include_title=True,
    )
    return pd.DataFrame(chunks)


def ingest_courses() -> Tuple[pd.DataFrame, object, object]:
    desc_path = DATA_SOURCES["courses_desc"]
    major_path = DATA_SOURCES["courses_major"]

    if not desc_path.exists() or not major_path.exists():
        raise FileNotFoundError("Course CSV files are missing.")

    # 1. ë‘ ë°ì´í„°ì…‹ ë¡œë“œ
    desc_df = pd.read_csv(desc_path).fillna("").astype(str)
    major_df = pd.read_csv(major_path).fillna("").astype(str)

    # 2. 'í•™ìˆ˜ë²ˆí˜¸' ê¸°ì¤€ìœ¼ë¡œ ë³‘í•© (Outer joinìœ¼ë¡œ ëˆ„ë½ ë°©ì§€)
    # suffixesëŠ” ì¶©ëŒ ì‹œ ë¶™ëŠ”ë°, ì—¬ê¸°ì„œëŠ” ì„œë¡œ ë‹¤ë¥¸ ì»¬ëŸ¼ì´ ë§Žì•„ ìœ ìš©í•¨.
    combined = pd.merge(major_df, desc_df, on="í•™ìˆ˜ë²ˆí˜¸", how="outer", suffixes=("", "_desc"))
    combined = combined.fillna("")

    # 3. ë©”íƒ€ë°ì´í„° ë° ê³µí†µ í•„ë“œ ì •ë¦¬
    combined["_source_table"] = "combined_statistics"
    combined["major"] = "í†µê³„í•™ê³¼"

    # --- ì´ìˆ˜ëŒ€ìƒ ë°ì´í„° ì •ì œ (ê²€ìƒ‰ ìš©ì´ì„±ì„ ìœ„í•´ ì •ê·œí™”) ---
    # ì˜ˆ: "í•™ì‚¬3,4ë…„" -> "3í•™ë…„, 4í•™ë…„", "í•™ì‚¬2ë…„" -> "2í•™ë…„"
    if "ì´ìˆ˜ëŒ€ìƒ" in combined.columns:
        def _normalize_grade(val: str) -> str:
            val = val.replace("í•™ì‚¬", "")
            if "," in val:
                # "3,4ë…„" -> "3,4" -> ["3", "4"] -> "3í•™ë…„, 4í•™ë…„"
                parts = val.replace("ë…„", "").split(",")
                return ", ".join([f"{p.strip()}í•™ë…„" for p in parts])
            else:
                # "2ë…„" -> "2í•™ë…„"
                return val.replace("ë…„", "í•™ë…„")
        
        combined["ì´ìˆ˜ëŒ€ìƒ"] = combined["ì´ìˆ˜ëŒ€ìƒ"].apply(_normalize_grade)

    session = SessionLocal()
    try:
        session.query(Chunk).filter(Chunk.course_id.isnot(None)).delete()
        session.query(Course).delete()
        session.commit()
        
        course_objs = []
        title_candidates = ["êµê³¼ëª©ëª…", "êµ­ë¬¸êµê³¼ëª©ëª…", "course_name", "title", "êµê³¼ëª©"]
        
        for _, row in combined.iterrows():
            # ì œëª© ì°¾ê¸° (êµê³¼ëª©ëª… ìš°ì„ , ì—†ìœ¼ë©´ êµ­ë¬¸êµê³¼ëª©ëª… ë“±)
            title = next((str(row.get(col, "")).strip() for col in title_candidates if str(row.get(col, "")).strip()), "í†µê³„í•™ê³¼ êµê³¼")
            code = str(row.get("í•™ìˆ˜ë²ˆí˜¸", "")).strip()
            
            # ì „ì²´ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì €ìž¥
            row_dict = row.to_dict()
            safe_dict = {k: str(v) for k, v in row_dict.items()}
            raw_json = json.dumps(safe_dict, ensure_ascii=False)
            
            # í•´ì„¤(Description) í•„ë“œ í™•ë³´
            description = str(row.get("í•´ì„¤", "")).strip()

            obj = Course(
                course_code=code, title=title, 
                source_table=row.get("_source_table"),
                raw_data=raw_json,
                description=description
            )
            course_objs.append(obj)
            
        session.add_all(course_objs)
        session.commit()
        combined["db_id"] = [obj.id for obj in course_objs]
    finally:
        session.close()
        
    chunks_df = build_course_chunks(combined)
    _save_chunks_to_sqlite(chunks_df, "courses")
    return _persist_chunks("courses", DATASET_ARTIFACTS["courses"].collection, chunks_df)


# --- Staff ---

def build_staff_chunks(df: pd.DataFrame) -> pd.DataFrame:
    docs = []
    exclude_cols = {"ì¡°ì§(íŠ¸ë¦¬)", "db_id", "raw_data"}
    
    for _, row in df.iterrows():
        # rowëŠ” [ì¡°ì§(íŠ¸ë¦¬), Data_0, Data_1, ...] í˜•íƒœ
        
        # 1. ì¡°ì§(íŠ¸ë¦¬) ì •ë³´
        dept = row.get("ì¡°ì§(íŠ¸ë¦¬)", "")
        
        # 2. ë‚˜ë¨¸ì§€ ë°ì´í„°
        info_parts = []
        phone_number = ""
        
        for col in df.columns:
            if col in exclude_cols or col.startswith("Unnamed"): continue
            val = str(row.get(col, "")).strip()
            if not val or val.lower() == "nan":
                continue
            
            # ì „í™”ë²ˆí˜¸ ê°ì§€ (ê°„ë‹¨í•œ íŒ¨í„´)
            if re.match(r'^\d{2,3}[-.]?\d{3,4}[-.]?\d{4}$', val):
                phone_number = val
            else:
                info_parts.append(val)
        
        content = " ".join(info_parts)
        
        # ì œëª©: ë¶€ì„œëª… - (ì²« ë²ˆì§¸ ë°ì´í„°: ë³´í†µ ì´ë¦„/ì§ìœ„)
        name_candidate = info_parts[0] if info_parts else "êµì§ì›"
        title = f"{dept} - {name_candidate}"
        
        full_text = f"ì†Œì†: {dept}\n\nì •ë³´: {content}"
        if phone_number:
            full_text += f"\n\nì „í™”ë²ˆí˜¸: {phone_number}"
        
        doc_id = make_doc_id("staff", dept, full_text)
        
        docs.append({
            "doc_id": doc_id,
            "title": title,
            "text": full_text,
            "topics": dept,
            "source": "staff",
            "staff_id": row.get("db_id"),
            "url": "",
            "published_at": "",
        })
        
    chunks = to_chunks(docs, chunk_size=CHUNK_SIZE, chunk_overlap=0, include_title=True)
    return pd.DataFrame(chunks)


def ingest_staff() -> Tuple[pd.DataFrame, object, object]:
    path = DATA_SOURCES["staff"]
    if not path.exists():
        print(f"âš ï¸ Staff CSV not found: {path}")
        return pd.DataFrame(), None, None

    df = pd.read_csv(path).fillna("").astype(str)
    
    session = SessionLocal()
    try:
        session.query(Chunk).filter(Chunk.staff_id.isnot(None)).delete()
        session.query(Staff).delete()
        session.commit()
        
        staff_objs = []
        for _, row in df.iterrows():
            raw_json = json.dumps(row.to_dict(), ensure_ascii=False)
            dept = row.get("ì¡°ì§(íŠ¸ë¦¬)", "")
            
            # ì´ë¦„ í•„ë“œ ì¶”ì • (ì²« ë²ˆì§¸ ë°ì´í„° ì»¬ëŸ¼ ì‚¬ìš©)
            # ì‹¤ì œ ì»¬ëŸ¼ëª…ì€ Data_0, Data_1...
            name_val = ""
            for col in df.columns:
                if col.startswith("Data_"):
                    val = row.get(col, "").strip()
                    if val:
                        name_val = val
                        break
            
            obj = Staff(
                department=dept,
                name=name_val,
                raw_data=raw_json
            )
            staff_objs.append(obj)
            
        session.add_all(staff_objs)
        session.commit()
        df["db_id"] = [obj.id for obj in staff_objs]
    finally:
        session.close()
        
    chunks_df = build_staff_chunks(df)
    _save_chunks_to_sqlite(chunks_df, "staff")
    return _persist_chunks("staff", DATASET_ARTIFACTS["staff"].collection, chunks_df)


def ingest_all() -> Dict[str, Tuple[pd.DataFrame, object, object]]:
    # DB í…Œì´ë¸” ìƒì„±/í™•ì¸
    init_db()
    
    results: Dict[str, Tuple[pd.DataFrame, object, object]] = {}
    # ìˆœì„œëŒ€ë¡œ ì‹¤í–‰
    results["notices"] = ingest_notices()
    results["rules"] = ingest_rules()
    results["schedule"] = ingest_schedule()
    results["courses"] = ingest_courses()
    results["staff"] = ingest_staff()
    return results


def reindex_from_db(target: str | None = None) -> Dict[str, Tuple[pd.DataFrame, object, object]]:
    """SQLite DBì— ì €ìž¥ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ChromaDB ì¸ë±ìŠ¤ì™€ TF-IDFë¥¼ ìž¬êµ¬ì¶•í•©ë‹ˆë‹¤."""
    session = SessionLocal()
    results = {}
    
    try:
        # 1. Notices
        if not target or target == "notices":
            print("ðŸ”„ Re-indexing notices from DB...")
            query = session.query(Chunk, Notice).join(Notice, Chunk.notice_id == Notice.id)
            data = []
            for chunk, notice in query.all():
                data.append({
                    "chunk_id": chunk.chunk_id,
                    "chunk_text": chunk.chunk_text,
                    "title": notice.title,
                    "topics": notice.board,
                    "published_at": notice.published_date,
                    "url": notice.detail_url,
                    "attachments": notice.attachments,
                    "source": "notices",
                    "notice_id": notice.id
                })
            if data:
                df = pd.DataFrame(data)
                results["notices"] = _persist_chunks("notices", DATASET_ARTIFACTS["notices"].collection, df)
        
        # 2. Rules
        if not target or target == "rules":
            print("ðŸ”„ Re-indexing rules from DB...")
            query = session.query(Chunk, Rule).join(Rule, Chunk.rule_id == Rule.id)
            data = []
            for chunk, rule in query.all():
                data.append({
                    "chunk_id": chunk.chunk_id,
                    "chunk_text": chunk.chunk_text,
                    "title": rule.filename,
                    "topics": "ê·œì •",
                    "relative_dir": rule.relative_dir,
                    "filename": rule.filename,
                    "source": "rules",
                    "url": "",
                    "published_at": "",
                    "rule_id": rule.id
                })
            if data:
                df = pd.DataFrame(data)
                results["rules"] = _persist_chunks("rules", DATASET_ARTIFACTS["rules"].collection, df)

        # 3. Schedule
        if not target or target == "schedule":
            print("ðŸ”„ Re-indexing schedule from DB...")
            query = session.query(Chunk, Schedule).join(Schedule, Chunk.schedule_id == Schedule.id)
            data = []
            for chunk, sch in query.all():
                data.append({
                    "chunk_id": chunk.chunk_id,
                    "chunk_text": chunk.chunk_text,
                    "title": sch.title,
                    "schedule_start": sch.start_date,
                    "schedule_end": sch.end_date,
                    "category": sch.category,
                    "department": sch.department,
                    "topics": sch.category or "schedule",
                    "source": "schedule",
                    "url": "",
                    "published_at": sch.start_date,
                    "schedule_id": sch.id
                })
            if data:
                df = pd.DataFrame(data)
                results["schedule"] = _persist_chunks("schedule", DATASET_ARTIFACTS["schedule"].collection, df)

        # 4. Courses
        if not target or target == "courses":
            print("ðŸ”„ Re-indexing courses from DB...")
            query = session.query(Chunk, Course).join(Course, Chunk.course_id == Course.id)
            data = []
            for chunk, course in query.all():
                try:
                    raw_data = json.loads(course.raw_data) if course.raw_data else {}
                except:
                    raw_data = {}
                
                data.append({
                    "chunk_id": chunk.chunk_id,
                    "chunk_text": chunk.chunk_text,
                    "title": course.title,
                    "course_code": course.course_code,
                    "source_table": course.source_table,
                    "topics": course.source_table,
                    "source": "courses",
                    "url": "",
                    "published_at": "",
                    "course_id": course.id,
                    "major": raw_data.get("major", "")
                })
            if data:
                df = pd.DataFrame(data)
                results["courses"] = _persist_chunks("courses", DATASET_ARTIFACTS["courses"].collection, df)

        # 5. Staff (New)
        if not target or target == "staff":
            print("ðŸ”„ Re-indexing staff from DB...")
            query = session.query(Chunk, Staff).join(Staff, Chunk.staff_id == Staff.id)
            data = []
            for chunk, staff in query.all():
                data.append({
                    "chunk_id": chunk.chunk_id,
                    "chunk_text": chunk.chunk_text,
                    "title": f"{staff.department} - {staff.name}",
                    "topics": staff.department,
                    "source": "staff",
                    "url": "",
                    "published_at": "",
                    "staff_id": staff.id
                })
            if data:
                df = pd.DataFrame(data)
                results["staff"] = _persist_chunks("staff", DATASET_ARTIFACTS["staff"].collection, df)

    finally:
        session.close()
        
    return results


def main() -> None:
    # CLI ì‹¤í–‰ ì‹œ ì´ˆê¸°í™”
    init_db()
    
    parser = argparse.ArgumentParser(description="RAG Data Ingestion Pipeline")
    parser.add_argument(
        "--target",
        type=str,
        choices=["notices", "rules", "schedule", "courses", "staff"],
        help="Specify a single dataset to ingest (e.g., notices). If omitted, all datasets are ingested.",
    )
    parser.add_argument(
        "--from-db",
        action="store_true",
        help="Rebuild index from SQLite database instead of raw CSV files.",
    )
    args = parser.parse_args()

    results = {}
    
    if args.from_db:
        print("ðŸš€ Starting Re-indexing from SQLite DB...")
        results = reindex_from_db(args.target)
    elif args.target:
        print(f"ðŸš€ Ingesting only: {args.target}")
        if args.target == "notices":
            results["notices"] = ingest_notices()
        elif args.target == "rules":
            results["rules"] = ingest_rules()
        elif args.target == "schedule":
            results["schedule"] = ingest_schedule()
        elif args.target == "courses":
            results["courses"] = ingest_courses()
        elif args.target == "staff":
            results["staff"] = ingest_staff()
    else:
        print("ðŸš€ Ingesting ALL datasets...")
        results = ingest_all()

    for key, (chunks_df, _, _) in results.items():
        print(f"âœ… {key}: {len(chunks_df)} chunks indexed")


if __name__ == "__main__":
    main()


__all__ = [
    "DATASET_ARTIFACTS",
    "build_notice_chunks",
    "build_rule_chunks",
    "build_schedule_chunks",
    "build_course_chunks",
    "build_staff_chunks",
    "ingest_notices",
    "ingest_rules",
    "ingest_schedule",
    "ingest_courses",
    "ingest_staff",
    "ingest_all",
]