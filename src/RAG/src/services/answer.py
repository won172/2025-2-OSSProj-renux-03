"""Answer generation helpers that wrap the OpenAI chat API."""
from __future__ import annotations

from functools import lru_cache
from typing import Tuple

import pandas as pd
from openai import OpenAI

from src.config import OPENAI_API_KEY, OPENAI_MODEL

ANSWER_PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ ë™êµ­ëŒ€í•™êµ ìº í¼ìŠ¤ RAG ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. \
ë°˜ë“œì‹œ ì•„ë˜ í…ìŠ¤íŠ¸ë§Œ ê·¼ê±°ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”. \
ìµœê·¼ ë‚ ì§œì˜ ê³µì§€ì‚¬í•­ë§Œ ë‹µë³€ì— í¬í•¨í•˜ì„¸ìš”. \
ëª¨í˜¸í•˜ë©´ "ì œê³µëœ ìë£Œì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”. \
ë‚ ì§œ/ì‹œê°„ì€ YYYY-MM-DD ë˜ëŠ” HH:MM í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”í•´ ì£¼ì„¸ìš”.\n\nì§ˆë¬¸: {question}\n\n[ê´€ë ¨ ê³µì§€]\n{context}\n\n[ì¶œë ¥ í˜•ì‹]\n- ğŸ“Œ í•µì‹¬ ìš”ì•½: (í•œ ì¤„)\n- ğŸ“… ì¼ì •/ë§ˆê°:\n- ğŸ“‹ ì¡°ê±´/ëŒ€ìƒ/ë°©ë²•(ìˆë‹¤ë©´):\n- ğŸ“ ì°¸ê³  ë§í¬: (ìµœëŒ€ 3ê°œ)\n- âš ï¸ ì£¼ì˜: (ìë£Œì—ì„œ ëª…í™•í•˜ì§€ ì•Šì€ ì ì´ ìˆìœ¼ë©´)\n\në‹µë³€ë§Œ ì‘ì„±í•˜ì„¸ìš”."""


@lru_cache(maxsize=1)
def get_client() -> OpenAI:
    if not OPENAI_API_KEY:
        raise RuntimeError("OPENAI_API_KEY is not configured.")
    return OpenAI(api_key=OPENAI_API_KEY)


def extract_title(text: str) -> str:
    if not isinstance(text, str) or not text.strip():
        return ""
    if text.startswith("[") and "]" in text:
        closing = text.find("]")
        if closing > 1:
            return text[1:closing].strip()
    return text.split("\n", 1)[0].strip()[:120]


def format_citations(df: pd.DataFrame) -> str:
    lines = []
    for _, row in df.iterrows():
        title = extract_title(row.get("chunk_text", ""))
        date = row.get("published_at")
        url = row.get("url")
        if url and date:
            lines.append(f"- {title} ({date}) â€” {url}")
        elif url:
            lines.append(f"- {title} â€” {url}")
        else:
            lines.append(f"- {title}")
    return "\n".join(lines)


def build_context(df: pd.DataFrame) -> str:
    return "\n\n---\n\n".join(df["chunk_text"].tolist())


def answer_with_citations(
    query: str,
    hits: pd.DataFrame,
    model_name: str = OPENAI_MODEL,
    temperature: float = 0.2,
) -> Tuple[str, str]:
    if hits.empty:
        return "ì œê³µëœ ìë£Œì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.", ""

    context = build_context(hits)
    citations = format_citations(hits)
    prompt = ANSWER_PROMPT_TEMPLATE.format(question=query, context=context)

    client = get_client()
    response = client.chat.completions.create(
        model=model_name,
        messages=[{"role": "user", "content": prompt}],
        temperature=temperature,
    )
    answer = response.choices[0].message.content.strip()
    return answer, citations


__all__ = ["answer_with_citations", "format_citations", "extract_title"]
