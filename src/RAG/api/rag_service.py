import functools
import logging
import re
import sys
import uuid
import json
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass
from typing import Dict, List, Tuple

sys.path.append(str(Path(__file__).resolve().parents[1]))

import pandas as pd
from fastapi import FastAPI, HTTPException
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, Field
from starlette.concurrency import run_in_threadpool

from src.config import (
    DEFAULT_TOP_K,
    HYBRID_ALPHA,
    MAX_CONTEXT_LENGTH,
    RECENCY_WEIGHT,
    VECTORIZER_DIR,
)
from src.database import SessionLocal, PendingItem, CustomKnowledge, Chunk, Notice, Schedule
from src.pipelines.ingest import (
    DATASET_ARTIFACTS,
    ingest_courses,
    ingest_notices,
    ingest_rules,
    ingest_schedule,
    ingest_staff, # ì¶”ê°€
)
from src.search.hybrid import load_tfidf, hybrid_search_with_meta
from src.services.answer import format_citations
from src.services.langchain_chat import generate_langchain_answer
from src.models.embedding import get_embedder, encode_texts
from src.services.router import route_query
from src.utils.date_parser import extract_date_range_from_query
from src.utils.query_expansion import expand_query
from src.utils.preprocess import make_doc_id
from src.vectorstore.chroma_client import upsert_items

app = FastAPI(
    title="ë™ë˜‘ì´",
    description="25-2 ì˜¤í”ˆì†ŒìŠ¤ì†Œí”„íŠ¸ì›¨ì–´í”„ë¡œì íŠ¸ íŒ€ Renuxì˜ ë™êµ­ëŒ€í•™êµ ìº í¼ìŠ¤ RAG ì–´ì‹œìŠ¤í„´íŠ¸ API ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.",
)

@app.get("/notifications")
async def notifications_dummy():
    return []

@app.options("/notifications")
async def notifications_options_dummy():
    return {}

@app.options("/token")
async def token_options_dummy():
    return {}

_DATASET_LOADERS = {
    "notices": ingest_notices,
    "rules": ingest_rules,
    "schedule": ingest_schedule,
    "courses": ingest_courses,
    "staff": ingest_staff, # ì¶”ê°€
}

@dataclass
class DatasetCache:
    chunks: pd.DataFrame
    vectorizer: object
    matrix: object
    chunk_path: Path
    chunk_mtime: float
    tfidf_mtime: float


_datasets: Dict[str, DatasetCache] = {}


class SourceChunk(BaseModel):
    source: str
    metadata: Dict
    snippet: str


class AskResponse(BaseModel):
    answer: str
    citations: str
    route: List[str]
    sources: List[SourceChunk]


class AskRequest(BaseModel):
    question: str = Field(..., description="ì‚¬ìš©ì ì§ˆë¬¸", alias="question")
    session_id: str | None = Field(None, description="ëŒ€í™” ì„¸ì…˜ ID (ì—†ìœ¼ë©´ ê¸°ë³¸ ì„¸ì…˜)", alias="sessionId")
    major: str | None = Field(None, description="ì‚¬ìš©ì í•™ê³¼") # ìƒˆë¡œ ì¶”ê°€

    class Config:
        populate_by_name = True


class SubmitRequest(BaseModel):
    source_type: str
    data: str





def _ensure_dataset(key: str) -> Tuple[pd.DataFrame, object, object]:
    artifacts = DATASET_ARTIFACTS.get(key)
    if artifacts is None:
        raise KeyError(f"Unsupported dataset '{key}'")
    
    chunk_path = artifacts.chunk_path
    csv_path = artifacts.csv_path
    vectorizer_path = VECTORIZER_DIR / f"{key}_tfidf.pkl"

    if not chunk_path.exists() and csv_path.exists():
        artifacts.chunk_path = csv_path
        chunk_path = csv_path

    chunk_mtime = chunk_path.stat().st_mtime if chunk_path.exists() else -1.0
    vectorizer_mtime = vectorizer_path.stat().st_mtime if vectorizer_path.exists() else -1.0

    cache = _datasets.get(key)
    if cache and cache.chunk_path == chunk_path and cache.chunk_mtime == chunk_mtime and cache.tfidf_mtime == vectorizer_mtime:
        return cache.chunks, cache.vectorizer, cache.matrix

    try:
        if chunk_path.exists() and vectorizer_path.exists():
            if chunk_path.suffix == ".csv":
                chunks_df = pd.read_csv(chunk_path)
            else:
                chunks_df = pd.read_parquet(chunk_path)
            vectorizer, matrix = load_tfidf(key)
        else:
            chunks_df, vectorizer, matrix = _DATASET_LOADERS[key]()
            chunk_path = DATASET_ARTIFACTS[key].chunk_path
            chunk_mtime = chunk_path.stat().st_mtime if chunk_path.exists() else -1.0
            vectorizer_mtime = (VECTORIZER_DIR / f"{key}_tfidf.pkl").stat().st_mtime if (VECTORIZER_DIR / f"{key}_tfidf.pkl").exists() else -1.0
    except FileNotFoundError:
        chunks_df, vectorizer, matrix = _DATASET_LOADERS[key]()
        chunk_path = DATASET_ARTIFACTS[key].chunk_path
        chunk_mtime = chunk_path.stat().st_mtime if chunk_path.exists() else -1.0
        vectorizer_path = VECTORIZER_DIR / f"{key}_tfidf.pkl"
        vectorizer_mtime = vectorizer_path.stat().st_mtime if vectorizer_path.exists() else -1.0

    _datasets[key] = DatasetCache(
        chunks=chunks_df,
        vectorizer=vectorizer,
        matrix=matrix,
        chunk_path=chunk_path,
        chunk_mtime=chunk_mtime,
        tfidf_mtime=vectorizer_mtime,
    )
    return chunks_df, vectorizer, matrix


@app.on_event("startup")
def bootstrap_artifacts() -> None:
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ë°ì´í„°ì…‹ê³¼ ë¶„ë¥˜ê¸° ë“± ì£¼ìš” ì•„í‹°íŒ©íŠ¸ë¥¼ ë¯¸ë¦¬ ë¡œë“œí•©ë‹ˆë‹¤."""
    logging.basicConfig(level=logging.INFO)
    
    for key in _DATASET_LOADERS:
        try:
            _ensure_dataset(key)
            logging.info(f"âœ… Dataset '{key}' successfully loaded.")
        except (KeyError, FileNotFoundError, ValueError) as exc:
            logging.error(f"âš ï¸ Failed to warmup dataset '{key}': {exc}", exc_info=True)
            # ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨ëŠ” ì‹¬ê°í•œ ë¬¸ì œì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
            # í•„ìš”ì— ë”°ë¼ ì—¬ê¸°ì„œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì¢…ë£Œì‹œí‚¤ëŠ” ë¡œì§ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            # Ex: raise RuntimeError(f"Critical failure loading dataset {key}") from exc

    try:
        logging.info("â³ Warming up embedding model...")
        get_embedder()
        logging.info("âœ… Embedding model warmup completed.")
    except Exception as exc:
        logging.warning(f"âš ï¸ Embedding model warmup failed: {exc}", exc_info=True)



@app.post("/admin/submit")
async def submit_pending(req: SubmitRequest):
    session = SessionLocal()
    try:
        item = PendingItem(
            source_type=req.source_type,
            data=req.data,
            status="pending"
        )
        session.add(item)
        session.commit()
        return {"status": "ok", "id": item.id}
    finally:
        session.close()


@app.get("/admin/pending")
async def list_pending():
    session = SessionLocal()
    try:
        items = session.query(PendingItem).filter(PendingItem.status == "pending").all()
        return items
    finally:
        session.close()


@app.get("/admin/items")
async def list_all_items():
    session = SessionLocal()
    try:
        items = session.query(PendingItem).order_by(PendingItem.created_at.desc()).all()
        return items
    finally:
        session.close()



@app.post("/admin/approve/{item_id}")
async def approve_pending(item_id: int):
    session = SessionLocal()
    try:
        logging.info(f"ğŸ‘‰ [Admin] Approving item ID: {item_id}")
        item = session.query(PendingItem).filter(PendingItem.id == item_id).first()
        if not item:
            logging.error(f"âŒ [Admin] Item not found: {item_id}")
            return {"status": "error", "message": "Item not found"}

        if item.source_type == "custom_knowledge":
            data = json.loads(item.data)
            logging.info(f"ğŸ“ [Admin] Processing custom knowledge: {data.get('question')}")

            # 1. Create CustomKnowledge
            ck = CustomKnowledge(
                question=data.get("question"),
                answer=data.get("answer"),
                category=data.get("category")
            )
            session.add(ck)
            session.commit()  # to get ID
            logging.info(f"âœ… [Admin] CustomKnowledge saved to DB. ID: {ck.id}")

            # 2. Create Chunk
            doc_id = make_doc_id("custom_knowledge", str(ck.id), ck.question[:20])
            text = f"ì§ˆë¬¸: {ck.question}\n\në‹µë³€: {ck.answer}"
            if ck.category:
                text = f"ì¹´í…Œê³ ë¦¬: {ck.category}\n{text}"

            chunk = Chunk(
                chunk_id=doc_id,
                chunk_text=text,
                custom_knowledge_id=ck.id
            )
            session.add(chunk)
            session.commit()
            logging.info(f"âœ… [Admin] Chunk saved to DB. Chunk ID: {doc_id}")

            # 3. Upsert to Chroma
            target_collection = "dongguk_notices"
            embedding = encode_texts([text])
            metadata = {
                "source": "custom_knowledge",
                "question": ck.question,
                "category": ck.category or "",
                "created_at": str(ck.created_at),
                "major": "common" # í•„í„°ë§ ìš°íšŒë¥¼ ìœ„í•œ ê¸°ë³¸ê°’
            }
            metadata = {k: (v if v is not None else "") for k, v in metadata.items()}

            upsert_items(
                name=target_collection,
                ids=[doc_id],
                documents=[text],
                metadatas=[metadata],
                embeddings=embedding
            )
            logging.info(f"âœ… [Admin] Upserted to ChromaDB")

            # 4. Trigger dataset reload to include new custom knowledge from DB
            try:
                # Invalidate cache for notices dataset to force reload from DB
                if "notices" in _datasets:
                    del _datasets["notices"] 
                _ensure_dataset("notices")
                logging.info(f"âœ… [Admin] Triggered dataset reload for 'notices' to include new CustomKnowledge.")
            except Exception as e:
                logging.error(f"âŒ [Admin] Failed to trigger dataset reload after CustomKnowledge approval: {e}")
                # Reload failure is not critical for DB commit, but RAG may not reflect changes immediately



            item.status = "approved"
            session.commit()
            logging.info(f"ğŸ‰ [Admin] Item {item_id} successfully approved.")

            return {"status": "approved", "chunk_id": doc_id}

        elif item.source_type == "event":
            data = json.loads(item.data)
            logging.info(f"ğŸ“… [Admin] Processing event: {data.get('title')}")
            
            # 1. Create Schedule
            sch = Schedule(
                title=data.get("title"),
                start_date=data.get("start_date"),
                end_date=data.get("end_date"),
                category="í•™ê³¼í–‰ì‚¬",
                department=data.get("department"),
                content=data.get("description"),
                is_manual=1
            )
            session.add(sch)
            session.commit()
            logging.info(f"âœ… [Admin] Schedule saved to DB. ID: {sch.id}")

            # 2. Create Chunk
            doc_id = make_doc_id("schedule", sch.start_date, sch.end_date, sch.content)
            
            date_str = f"{sch.start_date}"
            if sch.end_date and sch.end_date != sch.start_date:
                date_str += f" ~ {sch.end_date}"
            
            rich_text = f"í•™ê³¼í–‰ì‚¬: {sch.title}\n\n{sch.content}\n\nê¸°ê°„: {date_str}"
            if sch.department:
                rich_text += f"\n\nì£¼ê´€ë¶€ì„œ: {sch.department}"
            
            if data.get("location"):
                 rich_text += f"\n\nì¥ì†Œ: {data.get('location')}"

            chunk = Chunk(
                chunk_id=doc_id,
                chunk_text=rich_text,
                schedule_id=sch.id
            )
            session.add(chunk)
            session.commit()
            logging.info(f"âœ… [Admin] Chunk saved to DB. Chunk ID: {doc_id}")

            # 3. Upsert to Chroma
            target_collection = "dongguk_schedule"
            embedding = encode_texts([rich_text])
            metadata = {
                "source": "schedule",
                "title": sch.title,
                "schedule_start": sch.start_date,
                "schedule_end": sch.end_date,
                "category": sch.category,
                "department": sch.department,
                "published_at": sch.start_date # for date filtering
            }
            metadata = {k: (v if v is not None else "") for k, v in metadata.items()}

            upsert_items(
                name=target_collection,
                ids=[doc_id],
                documents=[rich_text],
                metadatas=[metadata],
                embeddings=embedding
            )
            logging.info(f"âœ… [Admin] Upserted to ChromaDB (Schedule)")

            # 4. Trigger dataset reload
            try:
                if "schedule" in _datasets:
                    del _datasets["schedule"] 
                _ensure_dataset("schedule")
                logging.info(f"âœ… [Admin] Triggered dataset reload for 'schedule'.")
            except Exception as e:
                logging.error(f"âŒ [Admin] Failed to trigger dataset reload: {e}")

            item.status = "approved"
            session.commit()
            return {"status": "approved", "chunk_id": doc_id}

        elif item.source_type == "announcement":
            data = json.loads(item.data)
            logging.info(f"ğŸ“¢ [Admin] Processing announcement: {data.get('title')}")
            
            # 1. Create Notice
            notice = Notice(
                board=data.get("department", "ê³µì§€ì‚¬í•­"),
                title=data.get("title"),
                category=data.get("category", "ì¼ë°˜"),
                published_date=data.get("date"),
                content=data.get("content"),
                is_manual=1
            )
            session.add(notice)
            session.commit()
            logging.info(f"âœ… [Admin] Notice saved to DB. ID: {notice.id}")

            # 2. Create Chunk
            doc_id = make_doc_id(notice.title, notice.board, notice.published_date)
            
            text_content = notice.content
            prefix_parts = []
            if notice.board:
                prefix_parts.append(f"ê²Œì‹œíŒ: {notice.board}")
            if notice.published_date:
                prefix_parts.append(f"ê²Œì‹œì¼: {notice.published_date}")
            
            if prefix_parts:
                text_content = f"[{', '.join(prefix_parts)}]\n\n{text_content}"

            chunk = Chunk(
                chunk_id=doc_id,
                chunk_text=text_content,
                notice_id=notice.id
            )
            session.add(chunk)
            session.commit()

            # 3. Upsert to Chroma
            target_collection = "dongguk_notices"
            embedding = encode_texts([text_content])
            metadata = {
                "source": "notices",
                "title": notice.title,
                "topics": notice.board,
                "published_at": notice.published_date,
                "category": notice.category
            }
            metadata = {k: (v if v is not None else "") for k, v in metadata.items()}
            
            upsert_items(
                name=target_collection,
                ids=[doc_id],
                documents=[text_content],
                metadatas=[metadata],
                embeddings=embedding
            )
            logging.info(f"âœ… [Admin] Upserted to ChromaDB (Notice)")

            # 4. Trigger reload
            try:
                if "notices" in _datasets:
                    del _datasets["notices"]
                _ensure_dataset("notices")
            except Exception as e:
                logging.error(f"âŒ [Admin] Failed to reload notices: {e}")

            item.status = "approved"
            session.commit()
            return {"status": "approved", "chunk_id": doc_id}
            
        else:
             item.status = "approved_manually" 
             session.commit()
             return {"status": "approved_manually"}

    except Exception as e:
        session.rollback()
        logging.error(f"ğŸ”¥ [Admin] Critical Error in approve_pending: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}
    finally:
        session.close()


@app.post("/admin/reject/{item_id}")
async def reject_pending(item_id: int):
    session = SessionLocal()
    try:
        item = session.query(PendingItem).filter(PendingItem.id == item_id).first()
        if item:
            item.status = "rejected"
            session.commit()
        return {"status": "rejected"}
    finally:
        session.close()


@app.post("/ask", response_model=AskResponse)
async def ask(req: AskRequest) -> AskResponse:
    raw_query = req.question.strip()
    if not raw_query:
        raise HTTPException(status_code=400, detail="ì§ˆë¬¸ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    # ì¿¼ë¦¬ í™•ì¥ ë¡œì§ ì ìš©
    query = expand_query(raw_query)
    logging.info(f"Original query: '{raw_query}', Expanded query: '{query}'")

    session_id = req.session_id or str(uuid.uuid4())

    # ë¡œê·¸ì— ì²˜ë¦¬ëœ ì§ˆë¬¸ê³¼ ì„¸ì…˜ IDë¥¼ ì¶œë ¥í•˜ì—¬ ë””ë²„ê¹…ì„ ë•ìŠµë‹ˆë‹¤.
    logging.info(f"session: '{session_id}'")

    user_major = req.major
    
    # --- ë‚ ì§œ ë° í•™ê³¼ í•„í„°ë§ ë¡œì§ ---
    final_where_filter: Dict = {}
    date_range = await run_in_threadpool(extract_date_range_from_query, query)
    
    # 1. í•™ê³¼ í•„í„°ë§ (ChromaDB where ì ˆ ì‚¬ìš©)
    if user_major and user_major != "Default": 
        final_where_filter["major"] = {"$eq": user_major}

    # ë¡œê¹… ì¶”ê°€ (ë””ë²„ê¹… ìš©ì´)
    logging.info(f"Applying ChromaDB filters: {final_where_filter}")
    
    route = await route_query(query)
    frames: List[pd.DataFrame] = []

    # ê° ë°ì´í„°ì…‹ë³„ë¡œ í•„í„°ë¥¼ ì ìš©
    for dataset in route:
        try:
            chunks_df, vectorizer, matrix = await run_in_threadpool(_ensure_dataset, dataset)
        except (KeyError, FileNotFoundError) as exc:
            raise HTTPException(status_code=500, detail=f"Dataset '{dataset}' unavailable: {exc}")

        artifacts = DATASET_ARTIFACTS[dataset]
        
        # ë°ì´í„°ì…‹ë³„ë¡œ ì ìš©ë  í•„í„° ì¡°ì •
        current_dataset_filter = final_where_filter.copy()
        
        # í•™ê³¼ í•„í„°: coursesë§Œ ì§€ì›
        if dataset != "courses":
            current_dataset_filter.pop("major", None)
            
        # í•„í„°ê°€ ë¹„ì–´ìˆìœ¼ë©´ Noneìœ¼ë¡œ ì„¤ì •
        final_filter = current_dataset_filter if current_dataset_filter else None
            
        search_func = functools.partial(
            hybrid_search_with_meta,
            collection_name=artifacts.collection,
            chunks_df=chunks_df,
            tfidf_vectorizer=vectorizer,
            tfidf_matrix=matrix,
            query=query,
            top_k=DEFAULT_TOP_K * 3,
            alpha=HYBRID_ALPHA,
            where_filter=final_filter,
        )
        hits = await run_in_threadpool(search_func)
        
        # 2. ë‚ ì§œ í•„í„°ë§ (Pandas DataFrame í›„ì²˜ë¦¬)
        # ChromaDBì˜ ë³µí•© ì—°ì‚°ì ì œí•œì„ í”¼í•˜ê¸° ìœ„í•´ ë©”ëª¨ë¦¬ ìƒì—ì„œ í•„í„°ë§
        if date_range and not hits.empty and dataset in ["notices", "schedule", "rules"]:
            start_date_str = date_range[0].strftime('%Y-%m-%d')
            end_date_str = date_range[1].strftime('%Y-%m-%d')
            
            # ë‚ ì§œ ì»¬ëŸ¼ í™•ì¸ (published_at ë˜ëŠ” updated_at)
            # schedule ë°ì´í„°ì…‹ì€ 'schedule_start' ë“±ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë‚˜ 
            # í˜„ì¬ ingest ë¡œì§ìƒ 'published_at'ì— ì‹œì‘ì¼ì´ ë§¤í•‘ë˜ì–´ ìˆìŒ.
            if "published_at" in hits.columns:
                # ë‚ ì§œ í˜•ì‹ ë³€í™˜ ë° í•„í„°ë§
                hits["_temp_date"] = pd.to_datetime(hits["published_at"], errors='coerce')
                # NaT(ë‚ ì§œ ì—†ìŒ)ëŠ” í•„í„°ë§ ëŒ€ìƒì—ì„œ ì œì™¸í• ì§€ í¬í•¨í• ì§€ ê²°ì •í•´ì•¼ í•¨. 
                # ì—¬ê¸°ì„œëŠ” ë‚ ì§œ ì§ˆë¬¸ì´ë¯€ë¡œ ë‚ ì§œê°€ ìˆëŠ” ê²ƒë§Œ ë‚¨ê¹€.
                hits = hits[
                    (hits["_temp_date"] >= start_date_str) & 
                    (hits["_temp_date"] <= end_date_str)
                ]
                hits.drop(columns=["_temp_date"], inplace=True)
                logging.info(f"Date filtered {dataset}: {len(hits)} remaining")

        logging.info(f"Dataset: {dataset}, Filter: {final_filter}, Hits: {len(hits)}")

        if not hits.empty:
            hits["dataset"] = dataset
            frames.append(hits)
    
    if not frames:
        logging.info("No search results found. Falling back to LLM with empty context.")
        merged = pd.DataFrame()
    else:
        merged = pd.concat(frames, ignore_index=True)
    
    if not merged.empty and "hybrid_score" in merged.columns:
        if "published_at" in merged.columns and "updated_at" in merged.columns:
             merged["sort_date"] = pd.to_datetime(merged["published_at"].fillna(merged["updated_at"]), errors='coerce')
        elif "published_at" in merged.columns:
            merged["sort_date"] = pd.to_datetime(merged["published_at"], errors='coerce')
        elif "updated_at" in merged.columns:
            merged["sort_date"] = pd.to_datetime(merged["updated_at"], errors='coerce')
        else:
            merged["sort_date"] = pd.NaT

        # sort_dateê°€ NaTì—¬ë„ ë°ì´í„°ë¥¼ ë²„ë¦¬ì§€ ì•Šë„ë¡ ìˆ˜ì •
        merged.dropna(subset=["hybrid_score"], inplace=True)
        
        if not merged.empty:
            min_hybrid = merged["hybrid_score"].min()
            max_hybrid = merged["hybrid_score"].max()
            if max_hybrid > min_hybrid:
                merged["norm_hybrid"] = (merged["hybrid_score"] - min_hybrid) / (max_hybrid - min_hybrid)
            else:
                merged["norm_hybrid"] = 1.0

            # ë‚ ì§œ ì ìˆ˜ ê³„ì‚°: ë‚ ì§œê°€ ìˆëŠ” í–‰ë§Œ ê³„ì‚°í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” 0ì  ì²˜ë¦¬
            valid_dates = merged["sort_date"].dropna()
            if not valid_dates.empty:
                min_date = valid_dates.min().timestamp()
                max_date = valid_dates.max().timestamp()
                
                # ë‚ ì§œê°€ ì—†ëŠ” í–‰ì€ ìµœí•˜ì (min_date)ìœ¼ë¡œ ì±„ì›€
                merged["sort_timestamp"] = merged["sort_date"].apply(lambda x: x.timestamp() if pd.notnull(x) else min_date)
                
                if max_date > min_date:
                    merged["norm_recency"] = (merged["sort_timestamp"] - min_date) / (max_date - min_date)
                else:
                    merged["norm_recency"] = 1.0
            else:
                # ë‚ ì§œ ì •ë³´ê°€ ì•„ì˜ˆ ì—†ëŠ” ë°ì´í„°ì…‹(ì˜ˆ: courses)ì¸ ê²½ìš° ìµœì‹ ì„± ì ìˆ˜ 0 ë˜ëŠ” 1ë¡œ í†µì¼ (í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë§Œ ë°˜ì˜ë¨)
                merged["norm_recency"] = 0.0
            
            merged["final_score"] = (1 - RECENCY_WEIGHT) * merged["norm_hybrid"] + RECENCY_WEIGHT * merged["norm_recency"]
            merged.sort_values(by="final_score", ascending=False, inplace=True)
        else:
            merged.sort_values(by="hybrid_score", ascending=False, inplace=True)

    merged = merged.head(DEFAULT_TOP_K).reset_index(drop=True)

    context_parts = []
    for idx, row in merged.iterrows():
        part = f"ë¬¸ì„œ {idx+1} [ì¶œì²˜: {row.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}]:\n"
        if row.get('title'):
            part += f"ì œëª©: {row.get('title')}\n"
        if row.get('published_at'): # ê³µì§€ì‚¬í•­, ì¼ì • ë“± ë‚ ì§œ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°
            part += f"ê²Œì‹œì¼: {row.get('published_at')}\n"
        if row.get('url'): # URL ì •ë³´ê°€ ìˆëŠ” ê²½ìš°
            part += f"URL: {row.get('url')}\n"
        part += f"ë‚´ìš©:\n{row['chunk_text']}\n"
        context_parts.append(part)
    
    context_text = "\n\n---\n\n".join(context_parts) if context_parts else "ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ëŒ€í™”ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."
    context_text = context_text[:MAX_CONTEXT_LENGTH] # ìµœëŒ€ ê¸¸ì´ ì œí•œ ìœ ì§€ 
    # LLMì—ê²Œ í˜„ì¬ ë‚ ì§œë¥¼ ì „ë‹¬í•˜ì—¬ "ì˜¤ëŠ˜", "ì´ë²ˆ í•™ê¸°" ë“±ì˜ í‘œí˜„ì„ í•´ì„í•˜ë„ë¡ ë•ìŠµë‹ˆë‹¤.
    from datetime import timedelta, timezone
    KST = timezone(timedelta(hours=9))
    current_date = datetime.now(KST).strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„ (KST)')
    answer = await generate_langchain_answer(
        question=query, 
        context=context_text, 
        session_id=session_id, 
        current_date=current_date
    )
    
    # í›„ì²˜ë¦¬: ë³¼ë“œì²´(**) ì„œì‹ ê°•ì œ ì œê±°
    answer = answer.replace("**", "")
    
    citations_raw = await run_in_threadpool(format_citations, merged)
    citations = re.sub(r'<[^>]+>', '', citations_raw)

    sources = [
        SourceChunk(
            source=row.get("dataset", ""),
            metadata={col: row.get(col) for col in row.index if col not in {"chunk_text", "dataset", "title", "hybrid_score", "sort_date", "norm_hybrid", "norm_recency", "final_score"}},
            snippet=row.get("chunk_text", ""),
        )
        for _, row in merged.iterrows()
    ]

    return AskResponse(answer=answer, citations=citations, route=route, sources=sources)


@app.get("/health")
def health() -> dict:
    status = {}
    for key in _DATASET_LOADERS:
        cache = _datasets.get(key)
        status[key] = 0 if cache is None else len(cache.chunks)
    return {"status": "ok", "datasets": status}
