import functools
import logging
import re
import sys
import uuid
import json
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass
from typing import Dict, List, Tuple

sys.path.append(str(Path(__file__).resolve().parents[1]))

import pandas as pd
from scipy.sparse import vstack
from fastapi import FastAPI, HTTPException
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, Field
from starlette.concurrency import run_in_threadpool

from src.config import (
    DEFAULT_TOP_K,
    HYBRID_ALPHA,
    MAX_CONTEXT_LENGTH,
    RECENCY_WEIGHT,
    VECTORIZER_DIR,
)
from src.database import SessionLocal, PendingItem, CustomKnowledge, Chunk, Notice, Schedule, init_db
from src.pipelines.ingest import (
    DATASET_ARTIFACTS,
    ingest_courses,
    ingest_notices,
    ingest_rules,
    ingest_schedule,
    ingest_staff, # ì¶”ê°€
)
from src.search.hybrid import load_tfidf, hybrid_search_with_meta
from src.services.answer import format_citations
from src.services.langchain_chat import generate_langchain_answer
from src.models.embedding import get_embedder, encode_texts
from src.services.router import route_query
from src.utils.date_parser import extract_date_range_from_query
from src.utils.query_expansion import expand_query
from src.utils.preprocess import make_doc_id
from src.vectorstore.chroma_client import upsert_items

app = FastAPI(
    title="ë™ë˜‘ì´",
    description="25-2 ì˜¤í”ˆì†ŒìŠ¤ì†Œí”„íŠ¸ì›¨ì–´í”„ë¡œì íŠ¸ íŒ€ Renuxì˜ ë™êµ­ëŒ€í•™êµ ìº í¼ìŠ¤ RAG ì–´ì‹œìŠ¤í„´íŠ¸ API ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.",
)

@app.get("/notifications")
async def notifications_dummy():
    return []

@app.options("/notifications")
async def notifications_options_dummy():
    return {}

@app.options("/token")
async def token_options_dummy():
    return {}

_DATASET_LOADERS = {
    "notices": ingest_notices,
    "rules": ingest_rules,
    "schedule": ingest_schedule,
    "courses": ingest_courses,
    "staff": ingest_staff, # ì¶”ê°€
}

@dataclass
class DatasetCache:
    chunks: pd.DataFrame
    vectorizer: object
    matrix: object
    chunk_path: Path
    chunk_mtime: float
    tfidf_mtime: float


_datasets: Dict[str, DatasetCache] = {}


class SourceChunk(BaseModel):
    source: str
    metadata: Dict
    snippet: str


class AskResponse(BaseModel):
    answer: str
    citations: str
    route: List[str]
    sources: List[SourceChunk]


class AskRequest(BaseModel):
    question: str = Field(..., description="ì‚¬ìš©ì ì§ˆë¬¸", alias="question")
    session_id: str | None = Field(None, description="ëŒ€í™” ì„¸ì…˜ ID (ì—†ìœ¼ë©´ ê¸°ë³¸ ì„¸ì…˜)", alias="sessionId")
    major: str | None = Field(None, description="ì‚¬ìš©ì í•™ê³¼") # ìƒˆë¡œ ì¶”ê°€

    class Config:
        populate_by_name = True


class SubmitRequest(BaseModel):
    source_type: str
    data: str




def _ensure_dataset(key: str) -> Tuple[pd.DataFrame, object, object]:
    artifacts = DATASET_ARTIFACTS.get(key)
    if artifacts is None:
        raise KeyError(f"Unsupported dataset '{key}'")
    
    chunk_path = artifacts.chunk_path
    csv_path = artifacts.csv_path
    vectorizer_path = VECTORIZER_DIR / f"{key}_tfidf.pkl"

    if not chunk_path.exists() and csv_path.exists():
        artifacts.chunk_path = csv_path
        chunk_path = csv_path

    chunk_mtime = chunk_path.stat().st_mtime if chunk_path.exists() else -1.0
    vectorizer_mtime = vectorizer_path.stat().st_mtime if vectorizer_path.exists() else -1.0

    cache = _datasets.get(key)
    if cache and cache.chunk_path == chunk_path and cache.chunk_mtime == chunk_mtime and cache.tfidf_mtime == vectorizer_mtime:
        return cache.chunks, cache.vectorizer, cache.matrix

    try:
        if chunk_path.exists() and vectorizer_path.exists():
            if chunk_path.suffix == ".csv":
                chunks_df = pd.read_csv(chunk_path)
            else:
                chunks_df = pd.read_parquet(chunk_path)
            vectorizer, matrix = load_tfidf(key)
        else:
            chunks_df, vectorizer, matrix = _DATASET_LOADERS[key]()
            chunk_path = DATASET_ARTIFACTS[key].chunk_path
            chunk_mtime = chunk_path.stat().st_mtime if chunk_path.exists() else -1.0
            vectorizer_mtime = (VECTORIZER_DIR / f"{key}_tfidf.pkl").stat().st_mtime if (VECTORIZER_DIR / f"{key}_tfidf.pkl").exists() else -1.0
    except FileNotFoundError:
        chunks_df, vectorizer, matrix = _DATASET_LOADERS[key]()
        chunk_path = DATASET_ARTIFACTS[key].chunk_path
        chunk_mtime = chunk_path.stat().st_mtime if chunk_path.exists() else -1.0
        vectorizer_path = VECTORIZER_DIR / f"{key}_tfidf.pkl"
        vectorizer_mtime = vectorizer_path.stat().st_mtime if vectorizer_path.exists() else -1.0

    _datasets[key] = DatasetCache(
        chunks=chunks_df,
        vectorizer=vectorizer,
        matrix=matrix,
        chunk_path=chunk_path,
        chunk_mtime=chunk_mtime,
        tfidf_mtime=vectorizer_mtime,
    )
    return chunks_df, vectorizer, matrix


def _add_to_dataset_cache(key: str, doc_id: str, text: str, metadata: Dict) -> None:
    """ìºì‹œëœ ë°ì´í„°ì…‹ì— ìƒˆ í•­ëª©ì„ ì ì§„ì ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤ (ì „ì²´ ë¦¬ë¡œë“œ ë°©ì§€)."""
    if key not in _datasets:
        # ìºì‹œì— ì—†ìœ¼ë©´ ë¡œë“œ (ì´ ì‹œì ì— ë¡œë“œí•˜ëŠ” ê²ƒì€ ì–´ì©” ìˆ˜ ì—†ìŒ, í•˜ì§€ë§Œ ì´í›„ì—ëŠ” ìºì‹œë¨)
        _ensure_dataset(key)
    
    cache = _datasets[key]
    
    # 1. DataFrameì— í–‰ ì¶”ê°€
    new_row = metadata.copy()
    new_row["chunk_id"] = doc_id
    new_row["chunk_text"] = text
    # ensure all columns exist
    for col in cache.chunks.columns:
        if col not in new_row:
            new_row[col] = None
            
    # pd.concat is better than append
    new_df = pd.DataFrame([new_row])
    # ê¸°ì¡´ ì»¬ëŸ¼ ìˆœì„œ ìœ ì§€ë¥¼ ìœ„í•´ reindex
    new_df = new_df.reindex(columns=cache.chunks.columns)
    
    cache.chunks = pd.concat([cache.chunks, new_df], ignore_index=True)
    
    # 2. TF-IDF ë§¤íŠ¸ë¦­ìŠ¤ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ì–´íœ˜ ì‚¬ì „ ì‚¬ìš©)
    # ì‹ ê·œ ë‹¨ì–´ëŠ” ë°˜ì˜ë˜ì§€ ì•Šì§€ë§Œ, ì „ì²´ ë¦¬ë¡œë“œë³´ë‹¤ ì›”ë“±íˆ ë¹ ë¦„
    new_vec = cache.vectorizer.transform([text])
    cache.matrix = vstack([cache.matrix, new_vec])
    
    logging.info(f"âš¡ Incremental update for '{key}': Added 1 item. New size: {len(cache.chunks)}")


@app.on_event("startup")
def bootstrap_artifacts() -> None:
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ë°ì´í„°ì…‹ê³¼ ë¶„ë¥˜ê¸° ë“± ì£¼ìš” ì•„í‹°íŒ©íŠ¸ë¥¼ ë¯¸ë¦¬ ë¡œë“œí•©ë‹ˆë‹¤."""
    logging.basicConfig(level=logging.INFO)
    
    # Ensure DB tables exist
    try:
        init_db()
        logging.info("âœ… Database tables initialized.")
    except Exception as e:
        logging.error(f"âŒ Failed to initialize database: {e}")
    
    for key in _DATASET_LOADERS:
        try:
            _ensure_dataset(key)
            logging.info(f"âœ… Dataset '{key}' successfully loaded.")
        except (KeyError, FileNotFoundError, ValueError) as exc:
            logging.error(f"âš ï¸ Failed to warmup dataset '{key}': {exc}", exc_info=True)
            # ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨ëŠ” ì‹¬ê°í•œ ë¬¸ì œì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
            # í•„ìš”ì— ë”°ë¼ ì—¬ê¸°ì„œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì¢…ë£Œì‹œí‚¤ëŠ” ë¡œì§ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            # Ex: raise RuntimeError(f"Critical failure loading dataset {key}") from exc

    try:
        logging.info("â³ Warming up embedding model...")
        get_embedder()
        logging.info("âœ… Embedding model warmup completed.")
    except Exception as exc:
        logging.warning(f"âš ï¸ Embedding model warmup failed: {exc}", exc_info=True)



@app.post("/admin/submit")
async def submit_pending(req: SubmitRequest):
    session = SessionLocal()
    try:
        item = PendingItem(
            source_type=req.source_type,
            data=req.data,
            status="pending"
        )
        session.add(item)
        session.commit()
        return {"status": "ok", "id": item.id}
    finally:
        session.close()


@app.get("/admin/pending")
async def list_pending():
    session = SessionLocal()
    try:
        items = session.query(PendingItem).filter(PendingItem.status == "pending").all()
        return items
    finally:
        session.close()


@app.get("/admin/items")
async def list_all_items():
    session = SessionLocal()
    try:
        items = session.query(PendingItem).order_by(PendingItem.created_at.desc()).all()
        logging.info(f"ğŸ“‹ [Admin] Listed {len(items)} items.")
        return items
    except Exception as e:
        logging.error(f"âŒ [Admin] Failed to list items: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        session.close()



@app.post("/admin/approve/{item_id}")
async def approve_pending(item_id: int):
    session = SessionLocal()
    try:
        logging.info(f"ğŸ‘‰ [Admin] Approving item ID: {item_id}")
        item = session.query(PendingItem).filter(PendingItem.id == item_id).first()
        if not item:
            logging.error(f"âŒ [Admin] Item not found: {item_id}")
            return {"status": "error", "message": "Item not found"}

        data = json.loads(item.data)
        
        # ê³µí†µ Notice ê°ì²´ ìƒì„± ì¤€ë¹„
        notice = None
        
        if item.source_type == "custom_knowledge":
            logging.info(f"ğŸ“ [Admin] Processing custom knowledge: {data.get('question')}")
            
            notice = Notice(
                board=data.get("category", "ê¸°íƒ€"), # e.g. í•™ê³¼ì •ë³´
                title=data.get("question"),
                category="FAQ",
                published_date=datetime.now().strftime("%Y-%m-%d"),
                content=data.get("answer"),
                is_manual=1
            )

        elif item.source_type == "event":
            logging.info(f"ğŸ“… [Admin] Processing event: {data.get('title')}")
            
            # ë‚´ìš©ì„ ìƒì„¸í•˜ê²Œ êµ¬ì„±
            content_parts = []
            if data.get("description"):
                content_parts.append(data.get("description"))
            
            date_str = f"ì¼ì‹œ: {data.get('start_date')}"
            if data.get("end_date") and data.get("end_date") != data.get("start_date"):
                date_str += f" ~ {data.get('end_date')}"
            content_parts.append(date_str)
            
            if data.get("location"):
                content_parts.append(f"ì¥ì†Œ: {data.get('location')}")
                
            full_content = "\n\n".join(content_parts)

            notice = Notice(
                board=data.get("department", "í•™ê³¼í–‰ì‚¬"),
                title=data.get("title"),
                category="í–‰ì‚¬",
                published_date=data.get("start_date"),
                content=full_content,
                is_manual=1
            )

        elif item.source_type == "announcement":
            logging.info(f"ğŸ“¢ [Admin] Processing announcement: {data.get('title')}")
            
            notice = Notice(
                board=data.get("department", "ê³µì§€ì‚¬í•­"),
                title=data.get("title"),
                category=data.get("category", "ì¼ë°˜"),
                published_date=data.get("date"),
                content=data.get("content"),
                is_manual=1
            )
        
        if notice:
            # 1. Save to DB (Notices table)
            session.add(notice)
            session.commit()
            logging.info(f"âœ… [Admin] Notice saved to DB. ID: {notice.id}")

            # 2. Create Chunk
            doc_id = make_doc_id(notice.title, notice.board, notice.published_date)

            # Check for collision
            existing_chunk = session.query(Chunk).filter(Chunk.chunk_id == doc_id).first()
            if existing_chunk:
                logging.warning(f"âš ï¸ [Admin] Chunk ID collision for {doc_id}. Appending random UUID.")
                doc_id = f"{doc_id}_{uuid.uuid4().hex[:8]}"
            
            text_content = notice.content
            prefix_parts = []
            if notice.board:
                prefix_parts.append(f"ê²Œì‹œíŒ: {notice.board}")
            if notice.category:
                prefix_parts.append(f"ë¶„ë¥˜: {notice.category}")
            if notice.published_date:
                prefix_parts.append(f"ê²Œì‹œì¼: {notice.published_date}")
            
            if prefix_parts:
                text_content = f"[{', '.join(prefix_parts)}]\n\n{text_content}"

            chunk = Chunk(
                chunk_id=doc_id,
                chunk_text=text_content,
                notice_id=notice.id
            )
            session.add(chunk)
            session.commit()

            # 3. Upsert to Chroma (dongguk_notices)
            target_collection = "dongguk_notices"
            embedding = encode_texts([text_content])
            metadata = {
                "source": "notices",
                "title": notice.title,
                "topics": notice.board,
                "published_at": notice.published_date,
                "category": notice.category
            }
            metadata = {k: (v if v is not None else "") for k, v in metadata.items()}
            
            upsert_items(
                name=target_collection,
                ids=[doc_id],
                documents=[text_content],
                metadatas=[metadata],
                embeddings=embedding
            )
            logging.info(f"âœ… [Admin] Upserted to ChromaDB (Notice)")

            # 4. Trigger reload
            try:
                if "notices" in _datasets:
                    del _datasets["notices"]
                _ensure_dataset("notices")
                logging.info(f"âœ… [Admin] Reloaded notices dataset.")
            except Exception as e:
                logging.error(f"âŒ [Admin] Failed to reload notices: {e}")

            item.status = "approved"
            session.commit()
            return {"status": "approved", "chunk_id": doc_id}

        else:
             item.status = "approved_manually" 
             session.commit()
             return {"status": "approved_manually"}

    except Exception as e:
        session.rollback()
        logging.error(f"ğŸ”¥ [Admin] Critical Error in approve_pending: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}
    finally:
        session.close()


@app.post("/admin/reject/{item_id}")
async def reject_pending(item_id: int):
    session = SessionLocal()
    try:
        item = session.query(PendingItem).filter(PendingItem.id == item_id).first()
        if item:
            item.status = "rejected"
            session.commit()
        return {"status": "rejected"}
    finally:
        session.close()


@app.post("/ask", response_model=AskResponse)
async def ask(req: AskRequest) -> AskResponse:
    raw_query = req.question.strip()
    if not raw_query:
        raise HTTPException(status_code=400, detail="ì§ˆë¬¸ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    # ì¿¼ë¦¬ í™•ì¥ ë¡œì§ ì ìš©
    query = expand_query(raw_query)
    logging.info(f"Original query: '{raw_query}', Expanded query: '{query}'")

    session_id = req.session_id or str(uuid.uuid4())

    # ë¡œê·¸ì— ì²˜ë¦¬ëœ ì§ˆë¬¸ê³¼ ì„¸ì…˜ IDë¥¼ ì¶œë ¥í•˜ì—¬ ë””ë²„ê¹…ì„ ë•ìŠµë‹ˆë‹¤.
    logging.info(f"session: '{session_id}'")

    user_major = req.major
    
    # --- ë‚ ì§œ ë° í•™ê³¼ í•„í„°ë§ ë¡œì§ ---
    final_where_filter: Dict = {}
    date_range = await run_in_threadpool(extract_date_range_from_query, query)
    
    # 1. í•™ê³¼ í•„í„°ë§ (ChromaDB where ì ˆ ì‚¬ìš©)
    if user_major and user_major != "Default": 
        final_where_filter["major"] = {"$eq": user_major}

    # ë¡œê¹… ì¶”ê°€ (ë””ë²„ê¹… ìš©ì´)
    logging.info(f"Applying ChromaDB filters: {final_where_filter}")
    
    route = await route_query(query)
    frames: List[pd.DataFrame] = []

    # ê° ë°ì´í„°ì…‹ë³„ë¡œ í•„í„°ë¥¼ ì ìš©
    for dataset in route:
        try:
            chunks_df, vectorizer, matrix = await run_in_threadpool(_ensure_dataset, dataset)
        except (KeyError, FileNotFoundError) as exc:
            raise HTTPException(status_code=500, detail=f"Dataset '{dataset}' unavailable: {exc}")

        artifacts = DATASET_ARTIFACTS[dataset]
        
        # ë°ì´í„°ì…‹ë³„ë¡œ ì ìš©ë  í•„í„° ì¡°ì •
        current_dataset_filter = final_where_filter.copy()
        
        # í•™ê³¼ í•„í„°: coursesë§Œ ì§€ì›
        if dataset != "courses":
            current_dataset_filter.pop("major", None)
            
        # í•„í„°ê°€ ë¹„ì–´ìˆìœ¼ë©´ Noneìœ¼ë¡œ ì„¤ì •
        final_filter = current_dataset_filter if current_dataset_filter else None
            
        search_func = functools.partial(
            hybrid_search_with_meta,
            collection_name=artifacts.collection,
            chunks_df=chunks_df,
            tfidf_vectorizer=vectorizer,
            tfidf_matrix=matrix,
            query=query,
            top_k=DEFAULT_TOP_K * 3,
            alpha=HYBRID_ALPHA,
            where_filter=final_filter,
        )
        hits = await run_in_threadpool(search_func)
        
        # 2. ë‚ ì§œ í•„í„°ë§ (Pandas DataFrame í›„ì²˜ë¦¬)
        # ChromaDBì˜ ë³µí•© ì—°ì‚°ì ì œí•œì„ í”¼í•˜ê¸° ìœ„í•´ ë©”ëª¨ë¦¬ ìƒì—ì„œ í•„í„°ë§
        if date_range and not hits.empty and dataset in ["notices", "schedule", "rules"]:
            start_date_str = date_range[0].strftime('%Y-%m-%d')
            end_date_str = date_range[1].strftime('%Y-%m-%d')
            
            # ë‚ ì§œ ì»¬ëŸ¼ í™•ì¸ (published_at ë˜ëŠ” updated_at)
            # schedule ë°ì´í„°ì…‹ì€ 'schedule_start' ë“±ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë‚˜ 
            # í˜„ì¬ ingest ë¡œì§ìƒ 'published_at'ì— ì‹œì‘ì¼ì´ ë§¤í•‘ë˜ì–´ ìˆìŒ.
            if "published_at" in hits.columns:
                # ë‚ ì§œ í˜•ì‹ ë³€í™˜ ë° í•„í„°ë§
                hits["_temp_date"] = pd.to_datetime(hits["published_at"], errors='coerce')
                # NaT(ë‚ ì§œ ì—†ìŒ)ëŠ” í•„í„°ë§ ëŒ€ìƒì—ì„œ ì œì™¸í• ì§€ í¬í•¨í• ì§€ ê²°ì •í•´ì•¼ í•¨. 
                # ì—¬ê¸°ì„œëŠ” ë‚ ì§œ ì§ˆë¬¸ì´ë¯€ë¡œ ë‚ ì§œê°€ ìˆëŠ” ê²ƒë§Œ ë‚¨ê¹€.
                hits = hits[
                    (hits["_temp_date"] >= start_date_str) & 
                    (hits["_temp_date"] <= end_date_str)
                ]
                hits.drop(columns=["_temp_date"], inplace=True)
                logging.info(f"Date filtered {dataset}: {len(hits)} remaining")

        logging.info(f"Dataset: {dataset}, Filter: {final_filter}, Hits: {len(hits)}")

        if not hits.empty:
            hits["dataset"] = dataset
            frames.append(hits)
    
    if not frames:
        logging.info("No search results found. Falling back to LLM with empty context.")
        merged = pd.DataFrame()
    else:
        merged = pd.concat(frames, ignore_index=True)
    
    if not merged.empty and "hybrid_score" in merged.columns:
        if "published_at" in merged.columns and "updated_at" in merged.columns:
             merged["sort_date"] = pd.to_datetime(merged["published_at"].fillna(merged["updated_at"]), errors='coerce')
        elif "published_at" in merged.columns:
            merged["sort_date"] = pd.to_datetime(merged["published_at"], errors='coerce')
        elif "updated_at" in merged.columns:
            merged["sort_date"] = pd.to_datetime(merged["updated_at"], errors='coerce')
        else:
            merged["sort_date"] = pd.NaT

        # sort_dateê°€ NaTì—¬ë„ ë°ì´í„°ë¥¼ ë²„ë¦¬ì§€ ì•Šë„ë¡ ìˆ˜ì •
        merged.dropna(subset=["hybrid_score"], inplace=True)
        
        if not merged.empty:
            min_hybrid = merged["hybrid_score"].min()
            max_hybrid = merged["hybrid_score"].max()
            if max_hybrid > min_hybrid:
                merged["norm_hybrid"] = (merged["hybrid_score"] - min_hybrid) / (max_hybrid - min_hybrid)
            else:
                merged["norm_hybrid"] = 1.0

            # ë‚ ì§œ ì ìˆ˜ ê³„ì‚°: ë‚ ì§œê°€ ìˆëŠ” í–‰ë§Œ ê³„ì‚°í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” 0ì  ì²˜ë¦¬
            valid_dates = merged["sort_date"].dropna()
            if not valid_dates.empty:
                min_date = valid_dates.min().timestamp()
                max_date = valid_dates.max().timestamp()
                
                # ë‚ ì§œê°€ ì—†ëŠ” í–‰ì€ ìµœí•˜ì (min_date)ìœ¼ë¡œ ì±„ì›€
                merged["sort_timestamp"] = merged["sort_date"].apply(lambda x: x.timestamp() if pd.notnull(x) else min_date)
                
                if max_date > min_date:
                    merged["norm_recency"] = (merged["sort_timestamp"] - min_date) / (max_date - min_date)
                else:
                    merged["norm_recency"] = 1.0
            else:
                # ë‚ ì§œ ì •ë³´ê°€ ì•„ì˜ˆ ì—†ëŠ” ë°ì´í„°ì…‹(ì˜ˆ: courses)ì¸ ê²½ìš° ìµœì‹ ì„± ì ìˆ˜ 0 ë˜ëŠ” 1ë¡œ í†µì¼ (í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë§Œ ë°˜ì˜ë¨)
                merged["norm_recency"] = 0.0
            
            merged["final_score"] = (1 - RECENCY_WEIGHT) * merged["norm_hybrid"] + RECENCY_WEIGHT * merged["norm_recency"]
            merged.sort_values(by="final_score", ascending=False, inplace=True)
        else:
            merged.sort_values(by="hybrid_score", ascending=False, inplace=True)

    merged = merged.head(DEFAULT_TOP_K).reset_index(drop=True)

    context_parts = []
    for idx, row in merged.iterrows():
        part = f"ë¬¸ì„œ {idx+1} [ì¶œì²˜: {row.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}]:\n"
        if row.get('title'):
            part += f"ì œëª©: {row.get('title')}\n"
        if row.get('published_at'): # ê³µì§€ì‚¬í•­, ì¼ì • ë“± ë‚ ì§œ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°
            part += f"ê²Œì‹œì¼: {row.get('published_at')}\n"
        if row.get('url'): # URL ì •ë³´ê°€ ìˆëŠ” ê²½ìš°
            part += f"URL: {row.get('url')}\n"
        part += f"ë‚´ìš©:\n{row['chunk_text']}\n"
        context_parts.append(part)
    
    context_text = "\n\n---\n\n".join(context_parts) if context_parts else "ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ëŒ€í™”ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."
    context_text = context_text[:MAX_CONTEXT_LENGTH] # ìµœëŒ€ ê¸¸ì´ ì œí•œ ìœ ì§€ 
    # LLMì—ê²Œ í˜„ì¬ ë‚ ì§œë¥¼ ì „ë‹¬í•˜ì—¬ "ì˜¤ëŠ˜", "ì´ë²ˆ í•™ê¸°" ë“±ì˜ í‘œí˜„ì„ í•´ì„í•˜ë„ë¡ ë•ìŠµë‹ˆë‹¤.
    from datetime import timedelta, timezone
    KST = timezone(timedelta(hours=9))
    current_date = datetime.now(KST).strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„ (KST)')
    answer = await generate_langchain_answer(
        question=query, 
        context=context_text, 
        session_id=session_id, 
        current_date=current_date
    )
    
    # í›„ì²˜ë¦¬: ë³¼ë“œì²´(**) ì„œì‹ ê°•ì œ ì œê±°
    answer = answer.replace("**", "")
    
    citations_raw = await run_in_threadpool(format_citations, merged)
    citations = re.sub(r'<[^>]+>', '', citations_raw)

    sources = [
        SourceChunk(
            source=row.get("dataset", ""),
            metadata={col: row.get(col) for col in row.index if col not in {"chunk_text", "dataset", "title", "hybrid_score", "sort_date", "norm_hybrid", "norm_recency", "final_score"}},
            snippet=row.get("chunk_text", ""),
        )
        for _, row in merged.iterrows()
    ]

    return AskResponse(answer=answer, citations=citations, route=route, sources=sources)


@app.get("/health")
def health() -> dict:
    status = {}
    for key in _DATASET_LOADERS:
        cache = _datasets.get(key)
        status[key] = 0 if cache is None else len(cache.chunks)
    return {"status": "ok", "datasets": status}
