# RAG 챗봇 프로젝트 분석 보고서 (v2)

## 1. 프로젝트 개요

이 프로젝트는 동국대학교 관련 정보(공지, 학칙, 학사일정, 교과과정)를 기반으로 사용자 질문에 답변하는 RAG(검색 증강 생성) 챗봇 시스템입니다.

최신 버전에서는 시스템의 안정성과 확장성을 크게 개선했습니다. FastAPI를 사용하여 API 서버를 구축했으며, 사용자의 질문 의도를 LLM을 통해 파악하여(Query Routing) 관련 데이터 소스에서 정보를 검색하고, 이 정보를 바탕으로 LangChain과 OpenAI의 LLM(GPT-4o mini)을 통해 최종 답변을 생성합니다.

특히, 이전 버전의 주요 문제점이었던 **세션 관리 부재**와 **메모리 누수** 문제를 해결했습니다. 이제 사용자별 대화 기록은 **Redis**를 통해 안정적으로 관리되며, API 스키마 역시 명확하게 정의되어 있습니다.

데이터 검색은 ChromaDB 벡터 데이터베이스를 활용한 **하이브리드 검색**(Dense-Vector 검색 + Sparse-TF-IDF 검색) 방식을 채택하여 정확도를 높였습니다. 데이터 인덱싱과 모델 학습은 별도의 오프라인 스크립트로 관리됩니다.

## 2. 실행 흐름

시스템은 크게 **오프라인 인덱싱**과 **온라인 서빙** 두 단계로 나뉩니다.

### 2.1. 오프라인 인덱싱 (`src/pipelines/ingest.py`)

1.  **데이터 로드:** `data/` 디렉토리의 CSV 파일(공지, 학칙, 학사일정, 교과과정)들을 Pandas DataFrame으로 읽어들입니다.
2.  **데이터 정제 및 변환:** 각 데이터 소스에 특화된 정제 과정을 거칩니다.
    *   **학사일정 (`build_schedule_chunks`):** 정규표현식을 사용해 본문 내용에서 '주관부서' 정보를 명확히 추출하고, 관련 없는 정보(예: 월/일 정보 컬럼)를 제외하여 더 깨끗하고 의미 있는 텍스트를 생성합니다.
    *   나머지 데이터(공지, 학칙, 교과과정)도 각각의 `build_*_chunks` 함수를 통해 구조화된 `Document` 형태로 변환됩니다.
3.  **텍스트 분할 (Chunking):**
    *   `KonlpyTextSplitter`를 사용하여 한국어 텍스트를 의미론적으로 더 정확하게 분할합니다.
    *   긴 텍스트는 설정된 크기(`CHUNK_SIZE=800`)에 맞춰 작은 조각으로 분할됩니다.
4.  **임베딩 및 인덱싱:**
    *   모든 문서 텍스트를 한국어 특화 모델인 `nlpai-lab/KURE-v1`을 사용하여 벡터로 변환(임베딩)합니다.
    *   변환된 벡터와 메타데이터를 `notices`, `rules`, `schedule`, `courses`라는 별개의 컬렉션으로 나누어 ChromaDB에 저장합니다. 이 과정은 `artifacts/db_chroma/` 디렉토리에 영구 저장됩니다.
5.  **TF-IDF 벡터라이저 학습:** 하이브리드 검색의 Sparse 검색을 위해, 각 데이터셋별로 TF-IDF 벡터라이저를 학습시키고 그 결과를 `artifacts/vectorizers/`에 저장합니다.

### 2.2. 온라인 서빙 (`api/rag_service.py`)

1.  **API 요청:** 사용자가 `/ask` 엔드포인트로 질문(`question`)과 선택적으로 `session_id`를 포함한 요청을 보냅니다.
2.  **세션 관리:**
    *   요청에 `session_id`가 있으면 해당 세션의 대화 기록을 이어가고, 없으면 새로운 UUID를 생성하여 신규 세션을 시작합니다.
    *   대화 기록은 **Redis**(`RedisChatMessageHistory`)에 저장되어 메모리 누수 없이 안정적으로 관리됩니다.
3.  **쿼리 라우팅 (`router.py`):**
    *   `ChatOpenAI` 모델(LLM)을 사용하여 사용자의 질문을 분석합니다.
    *   LLM은 사전에 정의된 각 데이터셋의 설명(`LLM_ROUTER_DESCRIPTIONS`)을 참고하여, 질문과 가장 관련 있는 데이터셋(들)의 이름을 JSON 형식으로 반환합니다. (예: "휴학 규정 알려줘" -> `["rules"]`)
4.  **하이브리드 검색 (`hybrid.py`):**
    *   라우팅된 각 데이터셋에 대해 하이브리드 검색을 수행합니다.
    *   **Dense 검색:** 사용자의 질문을 `KURE-v1` 모델로 임베딩하여 ChromaDB에서 벡터 유사도 검색을 수행합니다.
    *   **Sparse 검색:** 미리 학습된 TF-IDF 벡터라이저를 사용하여 키워드 기반 검색을 수행합니다.
    *   두 검색 점수를 가중치(`alpha`)를 두어 합산하고, 날짜 최신성 등을 반영한 최종 점수로 순위를 매겨 상위 K개의 문서를 선정합니다.
5.  **컨텍스트 생성:** 검색된 모든 문서의 텍스트를 하나로 합쳐 LLM에게 제공할 최종 컨텍스트를 만듭니다 (최대 8000자).
6.  **답변 생성 (`langchain_chat.py`):**
    *   개선된 시스템 프롬프트(가장 최신 날짜의 정보를 우선 사용하도록 지시 포함), 생성된 컨텍스트, 사용자 질문, 그리고 Redis에서 가져온 대화 기록을 `gpt-4o-mini` 모델에 전달합니다.
    *   LLM은 제공된 정보를 바탕으로 간결하고 명확한 한국어 답변을 생성합니다.
7.  **응답 반환:** `AskResponse` 모델에 따라 생성된 답변, 라우팅 경로, 그리고 출처 정보를 포함한 `sources` 객체 목록을 사용자에게 반환합니다. 각 `source`의 `snippet` 필드는 이제 검색된 청크의 전체 텍스트를 포함합니다.

## 3. 주요 변경사항 및 해결된 문제

이전 버전의 분석 보고서에서 제기되었던 심각한 문제들이 대부분 해결되었습니다.

- **[해결됨] 세션 관리 부재:** `session_id`를 동적으로 처리하고 Redis 기반의 `RedisChatMessageHistory`를 도입하여 사용자별 대화 기록을 안전하게 분리하고 영속적으로 관리합니다.
- **[해결됨] 메모리 누수 위험:** 대화 기록을 서버 메모리가 아닌 Redis에 저장함으로써 메모리 누수 문제를 근본적으로 해결했습니다.
- **[해결됨] 부정확한 API 응답 스키마:** `response_model`을 명확한 `AskResponse` Pydantic 모델로 지정하고, 그에 맞춰 응답 객체를 반환하도록 수정하여 API의 안정성과 예측 가능성을 확보했습니다.
- **[개선됨] 쿼리 라우팅:** 기존의 "ML 분류기 + 키워드" 방식 대신, LLM을 이용한 시맨틱 라우팅으로 변경하여 더 유연하고 정확하게 사용자 의도를 파악합니다.
- **[개선됨] 답변 및 출처 형식:** 사용자 요구에 맞춰 LLM 프롬프트를 수정하여 더 간결한 답변을 유도하고, 응답 모델을 변경하여 출처로 URL 목록만 제공하도록 단순화했습니다.

## 4. 주요 개선사항 및 결론

이전 분석 보고서에서 제안된 모든 주요 개선 사항이 성공적으로 반영되었습니다.

- **[해결됨] 불안정한 텍스트 분할 로직:** `KonlpyTextSplitter`를 도입하여, 한국어의 특성을 고려한 의미론적 텍스트 분할을 수행하도록 개선했습니다. 이를 통해 검색 정확도를 높일 것으로 기대됩니다.

- **[해결됨] 광범위한 예외 처리:** `router.py`와 `api/rag_service.py`의 예외 처리 블록을 수정하여, `ValidationError` 등 예상 가능한 특정 예외를 명시적으로 처리하고 오류 내용을 로깅하도록 변경했습니다.

- **[해결됨] 공지사항 카테고리 분류 모델 의존성:** 아키텍처 단순화를 위해 별도의 ML 모델을 호출하던 로직을 제거했습니다. 이제 시스템은 더 간결하고 예측 가능한 방식으로 동작합니다.

## 5. 결론

본 보고서 v2에서 제안된 주요 개선안들이 모두 성공적으로 시스템에 반영되었습니다.
- **아키텍처 단순화:** 불필요한 ML 모델 의존성을 제거했습니다.
- **안정성 향상:** 구체적인 예외 처리 로직을 적용했습니다.
- **성능 개선:** 한국어에 최적화된 텍스트 분할기를 도입하여 검색 정확도 향상을 기대할 수 있게 되었습니다.

현재 RAG 시스템은 이전보다 훨씬 안정적이고 유지보수하기 좋은 구조를 갖추게 되었습니다. 추가적인 최적화는 언제나 가능하겠지만, 보고서에서 지적된 핵심 문제들은 모두 해결되었습니다.