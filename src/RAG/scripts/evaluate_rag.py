import asyncio
import os
import json
import pandas as pd
from typing import List, Dict
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field

# RAG ì„œë¹„ìŠ¤ import (ë¡œì»¬ ì‹¤í–‰ ê°€ì •)
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).resolve().parents[1]))

# í•„ìš”í•œ ëª¨ë“ˆ import (ingest ë“± ì´ˆê¸°í™” í•„ìš”)
from src.database import init_db
from api.rag_service import ask, AskRequest, bootstrap_artifacts

# í‰ê°€ìš© LLM ì„¤ì •
EVAL_MODEL = "gpt-4o-mini" # ë¹„ìš© ì ˆê°ì„ ìœ„í•´ mini ì‚¬ìš©, ë” ì •í™•í•œ í‰ê°€ëŠ” gpt-4 ê¶Œì¥

# í‰ê°€ ê²°ê³¼ ìŠ¤í‚¤ë§ˆ
class EvalResult(BaseModel):
    score: int = Field(description="1 to 5 score")
    reasoning: str = Field(description="Reasoning for the score")

parser = JsonOutputParser(pydantic_object=EvalResult)

# í‰ê°€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
CORRECTNESS_PROMPT = PromptTemplate(
    template="""ë‹¹ì‹ ì€ RAG ì‹œìŠ¤í…œì˜ ë‹µë³€ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì±„ì ê´€ì…ë‹ˆë‹¤.
    
[ì§ˆë¬¸]: {question}
[ê¸°ì¤€ ë‹µë³€]: {ground_truth}
[ìƒì„±ëœ ë‹µë³€]: {generated_answer}

ìƒì„±ëœ ë‹µë³€ì´ ê¸°ì¤€ ë‹µë³€ì˜ í•µì‹¬ ì •ë³´ë¥¼ ì •í™•í•˜ê²Œ í¬í•¨í•˜ê³  ìˆëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.
1ì (ì „í˜€ ë‹¤ë¦„)ë¶€í„° 5ì (ì™„ë²½í•˜ê²Œ ì¼ì¹˜)ê¹Œì§€ ì ìˆ˜ë¥¼ ë§¤ê¸°ê³  ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.

{format_instructions}
""",
    input_variables=["question", "ground_truth", "generated_answer"],
    partial_variables={"format_instructions": parser.get_format_instructions()}
)

FAITHFULNESS_PROMPT = PromptTemplate(
    template="""ë‹¹ì‹ ì€ RAG ì‹œìŠ¤í…œì˜ ë‹µë³€ì´ ê²€ìƒ‰ëœ ë¬¸ì„œì— ê¸°ë°˜í–ˆëŠ”ì§€ í‰ê°€í•˜ëŠ” ì±„ì ê´€ì…ë‹ˆë‹¤.

[ê²€ìƒ‰ëœ ë¬¸ë§¥]:
{context}

[ìƒì„±ëœ ë‹µë³€]:
{generated_answer}

ìƒì„±ëœ ë‹µë³€ì´ ì˜¤ì§ ì œê³µëœ ë¬¸ë§¥ì—ë§Œ ê¸°ë°˜í•˜ì—¬ ì‘ì„±ë˜ì—ˆëŠ”ì§€, ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì„ ì§€ì–´ë‚´ì§€(Hallucination) ì•Šì•˜ëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.
1ì (ë¬¸ë§¥ê³¼ ì „í˜€ ìƒê´€ì—†ìŒ/í—ˆìœ„ì •ë³´)ë¶€í„° 5ì (ë¬¸ë§¥ì— ì™„ë²½íˆ ê¸°ë°˜í•¨)ê¹Œì§€ ì ìˆ˜ë¥¼ ë§¤ê¸°ê³  ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.

{format_instructions}
""",
    input_variables=["context", "generated_answer"],
    partial_variables={"format_instructions": parser.get_format_instructions()}
)

async def evaluate_single(item: Dict, llm: ChatOpenAI):
    question = item["question"]
    ground_truth = item["ground_truth"]
    
    print(f"ğŸ” Evaluating: {question}")
    
    # 1. RAG ì‹¤í–‰
    try:
        response = await ask(AskRequest(question=question))
        generated_answer = response.answer
        # ê²€ìƒ‰ëœ ë¬¸ë§¥ ì¡°í•©
        context = "\n".join([f"- {src.snippet}" for src in response.sources])
        
        if not context:
            context = "ê²€ìƒ‰ëœ ë¬¸ì„œ ì—†ìŒ"
            
    except Exception as e:
        print(f"âŒ Error during RAG generation: {e}")
        return None

    # 2. LLM í‰ê°€ (Correctness)
    try:
        correctness_chain = CORRECTNESS_PROMPT | llm | parser
        correctness_result = await correctness_chain.ainvoke({
            "question": question,
            "ground_truth": ground_truth,
            "generated_answer": generated_answer
        })
    except Exception as e:
        print(f"âš ï¸ Correctness eval failed: {e}")
        correctness_result = {"score": 0, "reasoning": "Eval Failed"}
    
    # 3. LLM í‰ê°€ (Faithfulness)
    try:
        faithfulness_chain = FAITHFULNESS_PROMPT | llm | parser
        faithfulness_result = await faithfulness_chain.ainvoke({
            "context": context[:10000], # í† í° ì œí•œ ê³ ë ¤í•˜ì—¬ ìë¦„
            "generated_answer": generated_answer
        })
    except Exception as e:
        print(f"âš ï¸ Faithfulness eval failed: {e}")
        faithfulness_result = {"score": 0, "reasoning": "Eval Failed"}

    return {
        "question": question,
        "generated_answer": generated_answer,
        "ground_truth": ground_truth,
        "correctness_score": correctness_result.get("score", 0),
        "correctness_reason": correctness_result.get("reasoning", ""),
        "faithfulness_score": faithfulness_result.get("score", 0),
        "faithfulness_reason": faithfulness_result.get("reasoning", ""),
        "retrieved_docs_count": len(response.sources)
    }

async def main():
    # ì´ˆê¸°í™”
    init_db()
    bootstrap_artifacts()

    # 1. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ì§ˆë¬¸ìœ¼ë¡œ ì¬êµ¬ì„±)
    test_dataset = [
        # --- í•™ì‚¬ì¼ì • (Schedule - dongguk_schedule.csv ê¸°ë°˜) ---
        {
            "question": "2025í•™ë…„ë„ 1í•™ê¸° ê°œê°•ì¼ì€ ì–¸ì œì•¼?",
            "ground_truth": "2025ë…„ 3ì›” 4ì¼ì…ë‹ˆë‹¤."
        },
        {
            "question": "2025ë…„ 1í•™ê¸° ìˆ˜ê°•ì‹ ì²­ í™•ì¸ ë° ì •ì • ê¸°ê°„ ì•Œë ¤ì¤˜",
            "ground_truth": "2025ë…„ 3ì›” 4ì¼ë¶€í„° 3ì›” 10ì¼ê¹Œì§€ì…ë‹ˆë‹¤."
        },
        {
            "question": "ì—¬ë¦„ë°©í•™(í•˜ê³„ë°©í•™) ì‹œì‘ì¼ì€ ì–¸ì œì•¼?",
            "ground_truth": "2025ë…„ 6ì›” 23ì¼ì…ë‹ˆë‹¤."
        },
        {
            "question": "2025ë…„ ë¶€ì²˜ë‹˜ì˜¤ì‹ ë‚ ì€ ì–¸ì œì•¼? ìˆ˜ì—… í•´?",
            "ground_truth": "2025ë…„ 5ì›” 5ì¼ì´ë©°, ê³µíœ´ì¼ì´ë¯€ë¡œ ìˆ˜ì—…ì´ ì—†ìŠµë‹ˆë‹¤. (ë³´ê°•ì¼ ì§€ì • ê°€ëŠ¥ì„± ìˆìŒ)"
        },

        # --- í•™ì¹™ (Rules - dongguk_rule_texts.csv ê¸°ë°˜) ---
        {
            "question": "ì¼ë°˜ íœ´í•™ì€ ìµœëŒ€ ëª‡ ë…„ê¹Œì§€ í•  ìˆ˜ ìˆì–´?",
            "ground_truth": "ì¼ë°˜íœ´í•™ ê¸°ê°„ì€ 1íšŒì— 1ë…„(2ê°œ í•™ê¸°) ì´ë‚´ë¡œ í•˜ë©°, ì¬í•™ ì¤‘ í†µì‚° 3ë…„(6ê°œ í•™ê¸°)ì„ ì´ˆê³¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        },
        {
            "question": "ì¡°ê¸°ì¡¸ì—… í•˜ë ¤ë©´ ì„±ì ì´ ì–¼ë§ˆë‚˜ ë˜ì–´ì•¼ í•´?",
            "ground_truth": "6í•™ê¸° ë˜ëŠ” 7í•™ê¸° ì´ìˆ˜ í›„ ì¡¸ì—…ìš”ê±´ì„ ê°–ì¶”ê³ , ì´ í‰ì í‰ê· ì´ 4.0 ì´ìƒì´ì–´ì•¼ ì¡°ê¸°ì¡¸ì—…ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
        },
        {
            "question": "ì„±ì ê²½ê³ (í•™ì‚¬ê²½ê³ ) ê¸°ì¤€ì´ ë­ì•¼?",
            "ground_truth": "ë§¤ í•™ê¸° ì„±ì  í‰ì í‰ê· ì´ 1.75 ë¯¸ë§Œì¸ ê²½ìš° ì„±ì ê²½ê³ ë¥¼ ë°›ìŠµë‹ˆë‹¤."
        },
        {
            "question": "ì „ê³¼(ì†Œì†ë³€ê²½) ì‹ ì²­ ìê²©ì€ ì–´ë–»ê²Œ ë¼?",
            "ground_truth": "2í•™ë…„ ë˜ëŠ” 3í•™ë…„ ì§„ê¸‰ ì˜ˆì •ìë¡œì„œ, ì´ í‰ì í‰ê·  3.0 ì´ìƒì´ì–´ì•¼ ì‹ ì²­ ê°€ëŠ¥í•©ë‹ˆë‹¤."
        },

        # --- êµê³¼ëª© (Courses - í†µê³„í•™ê³¼ ë°ì´í„° ê¸°ë°˜) ---
        {
            "question": "'íƒìƒ‰ì ìë£Œë¶„ì„' ê³¼ëª©ì˜ í•™ìˆ˜ë²ˆí˜¸ ì•Œë ¤ì¤˜",
            "ground_truth": "STA2005 ì…ë‹ˆë‹¤."
        },
        {
            "question": "ìˆ˜ë¦¬í†µê³„í•™1 ìˆ˜ì—…ì€ ëª‡ í•™ì ì´ì•¼?",
            "ground_truth": "3í•™ì ì…ë‹ˆë‹¤."
        },
        {
            "question": "í†µê³„í•™ê³¼ 2í•™ë…„ì´ ë“¤ì„ë§Œí•œ ì „ê³µ ê¸°ì´ˆ ê³¼ëª© ì¶”ì²œí•´ì¤˜",
            "ground_truth": "íƒìƒ‰ì ìë£Œë¶„ì„(STA2005), í™•ë¥ ê³¼ì •ë¡ (STA2015), ìˆ˜ë¦¬í†µê³„í•™1(STA2017) ë“±ì´ 2í•™ë…„ ëŒ€ìƒ ê¸°ì´ˆ ê³¼ëª©ì…ë‹ˆë‹¤."
        },
        {
            "question": "'íšŒê·€í•´ì„' ê³¼ëª©ì€ ì˜ì–´ë¡œ ìˆ˜ì—…í•´?",
            "ground_truth": "ë„¤, ì›ì–´ê°•ì˜(ì˜ì–´)ë¡œ ì§„í–‰ë˜ëŠ” ê³¼ëª©ì…ë‹ˆë‹¤."
        },
        {
            "question": "ìˆ˜ë¦¬í†µê³„í•™2ì˜ ì„ ìˆ˜ê³¼ëª©ì´ ìˆì–´?",
            "ground_truth": "ëŒ€í•™í†µê³„ë°ì‹¤ìŠµ1, ëŒ€í•™í†µê³„ë°ì‹¤ìŠµ2, ìˆ˜ë¦¬í†µê³„í•™1 ì´ ì„ ìˆ˜ê¶Œì¥ ê³¼ëª©ì…ë‹ˆë‹¤."
        },

        # --- êµì§ì›/ë¶€ì„œ (Staff - dongguk_staff_contacts.csv ê¸°ë°˜) ---
        {
            "question": "í•™ì‚¬ì§€ì›íŒ€ ì „í™”ë²ˆí˜¸ê°€ ë­ì•¼?",
            "ground_truth": "054-770-2033 (ë˜ëŠ” ê²€ìƒ‰ëœ í•™ì‚¬ì§€ì›íŒ€ ë²ˆí˜¸)"
        },
        {
            "question": "ì¥í•™íŒ€ ìœ„ì¹˜ ì•Œë ¤ì¤˜",
            "ground_truth": "ë³¸ê´€ 1ì¸µ ë“± (ë°ì´í„°ì— ìœ„ì¹˜ ì •ë³´ê°€ ìˆë‹¤ë©´)"
        },
        {
            "question": "í•™ìƒìƒë‹´ì„¼í„°ì—ì„œëŠ” ë¬´ìŠ¨ ì¼ì„ í•´?",
            "ground_truth": "í•™ìƒë“¤ì˜ ì‹¬ë¦¬ ìƒë‹´, ì§„ë¡œ ìƒë‹´ ë“±ì˜ ì—…ë¬´ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤."
        },

        # --- ê³µì§€ì‚¬í•­ (Notices - ìµœê·¼ ê³µì§€ ê¸°ë°˜) ---
        {
            "question": "2025í•™ë…„ë„ ì‹ ì…ìƒ ë“±ë¡ê¸ˆ ë‚©ë¶€ ê¸°ê°„ì€?",
            "ground_truth": "2025ë…„ 2ì›” ì¤‘ ì§€ì •ëœ ê¸°ê°„ (ê³µì§€ì‚¬í•­ ë‚´ìš© ì°¸ì¡°)"
        },
        {
            "question": "ì¡¸ì—…ì•¨ë²” ì´¬ì˜ ì¼ì • ë‚˜ì™”ì–´?",
            "ground_truth": "ê³µì§€ì‚¬í•­ì— ì¡¸ì—…ì•¨ë²” ì´¬ì˜ ê´€ë ¨ ì•ˆë‚´ê°€ ìˆë‹¤ë©´ í•´ë‹¹ ë‚ ì§œì™€ ì¥ì†Œë¥¼ ì•ˆë‚´í•©ë‹ˆë‹¤."
        },

        # --- ì¼ìƒ ëŒ€í™” (Chit-chat) ---
        {
            "question": "ì•ˆë…•, ë„ˆëŠ” ëˆ„êµ¬ë‹ˆ?",
            "ground_truth": "ì €ëŠ” ë™êµ­ëŒ€í•™êµ ì¬í•™ìƒì„ ìœ„í•œ ë§ì¶¤í˜• ì •ë³´ ì œê³µ ì±—ë´‡ 'ë™ë˜‘ì´'ì…ë‹ˆë‹¤."
        },
        {
            "question": "ë°˜ê°€ì›Œ",
            "ground_truth": "ë°˜ê°‘ìŠµë‹ˆë‹¤! í•™êµ ìƒí™œì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹ ê°€ìš”?"
        }
    ]

    llm = ChatOpenAI(model=EVAL_MODEL, temperature=0)
    
    results = []
    for item in test_dataset:
        result = await evaluate_single(item, llm)
        if result:
            results.append(result)
    
    # ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥
    if results:
        df = pd.DataFrame(results)
        print("\nğŸ“Š Evaluation Results:")
        print(df[["question", "correctness_score", "faithfulness_score"]])
        print(f"\nAverage Correctness: {df['correctness_score'].mean():.2f}")
        print(f"Average Faithfulness: {df['faithfulness_score'].mean():.2f}")
        
        output_file = "rag_evaluation_report.csv"
        df.to_csv(output_file, index=False, encoding="utf-8-sig")
        print(f"\nâœ… ìƒì„¸ ë¦¬í¬íŠ¸ ì €ì¥ë¨: {output_file}")
    else:
        print("í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main())
